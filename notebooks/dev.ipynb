{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7af612-e71d-4dfc-a477-088a7f79c1b6",
   "metadata": {},
   "source": [
    "# Development Notebook for Cortical Crowding Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8a076-56ce-45b0-950d-b70220b4d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuropythy as ny\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize\n",
    "from scipy.stats import gmean\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328aa4ac-48d3-46ba-8748-e8c287d3edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to be able to load in libraries that are in this repository's src directory,\n",
    "# so we add src to the system path:\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Now we can import corticalcrowding from the src directory:\n",
    "import corticalcrowding as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/crowding/crowding_data_withID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838bf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids_NYU = [\n",
    "    'sub-wlsubj070',\n",
    "    'sub-wlsubj114',\n",
    "    'sub-wlsubj121',\n",
    "    'sub-wlsubj135']\n",
    "sids_NEI = [ \n",
    "    'sub-wlsubj119',\n",
    "    'sub-wlsubj127',\n",
    "    'sub-wlsubj136',\n",
    "    'sub-wlsubj137',\n",
    "    'sub-wlsubj143',\n",
    "    'sub-wlsubj144',\n",
    "    'sub-wlsubj145',\n",
    "    'sub-wlsubj146',\n",
    "    'sub-wlsubj147',\n",
    "    'sub-wlsubj148',\n",
    "    'sub-wlsubj149',\n",
    "    'sub-wlsubj150',\n",
    "    'sub-wlsubj151',\n",
    "    'sub-wlsubj152',\n",
    "    'sub-wlsubj153',\n",
    "    'sub-wlsubj154',\n",
    "    'sub-wlsubj155',\n",
    "    'sub-wlsubj156',\n",
    "    'sub-wlsubj157',\n",
    "    'sub-wlsubj158',\n",
    "    'sub-wlsubj159',\n",
    "    'sub-wlsubj160',\n",
    "    'sub-wlsubj161',\n",
    "    'sub-wlsubj162',\n",
    "    'sub-wlsubj163',\n",
    "    'sub-wlsubj164',\n",
    "    'sub-wlsubj165',\n",
    "    'sub-wlsubj166',\n",
    "    'sub-wlsubj167',\n",
    "    'sub-wlsubj168',\n",
    "    'sub-wlsubj170',\n",
    "    'sub-wlsubj171',\n",
    "    'sub-wlsubj172',\n",
    "    'sub-wlsubj173',\n",
    "    'sub-wlsubj174',\n",
    "    'sub-wlsubj175',\n",
    "    'sub-wlsubj176']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids_orig = sids_NYU + sids_NEI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c2085",
   "metadata": {},
   "source": [
    "## crowding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 sessions, so now each sub have 12 cd (match with polar angle) instead of 24\n",
    "mean_cd_polar = df.groupby(['ID','Eccen_X','Eccen_Y'])['CrowdingDistance'].apply(gmean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each subject has 1 cd value at each eccentricity\n",
    "mean_cd = df.groupby(['ID','RadialEccentricity'])['CrowdingDistance'].apply(gmean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_list = df['CrowdingDistance'].tolist()\n",
    "mean_cd_list=mean_cd['CrowdingDistance'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 dfs based on eccen\n",
    "mean_1 = mean_cd[mean_cd['RadialEccentricity']==2.5]\n",
    "m_1 = mean_1['CrowdingDistance'].mean()\n",
    "st_1 = mean_1['CrowdingDistance'].std()\n",
    "mean_2 = mean_cd[mean_cd['RadialEccentricity']==5]\n",
    "m_2 = mean_2['CrowdingDistance'].mean()\n",
    "st_2 = mean_2['CrowdingDistance'].std()\n",
    "mean_3 = mean_cd[mean_cd['RadialEccentricity']==10]\n",
    "m_3 = mean_3['CrowdingDistance'].mean()\n",
    "st_3 = mean_3['CrowdingDistance'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecc = df['RadialEccentricity'].tolist()\n",
    "mean_x_ecc = mean_cd['RadialEccentricity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, _ = curve_fit(cc.crowding.func_cd, x_ecc, np.log10(cd_list), p0=0.15)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21810d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = [m_1, m_2, m_3]\n",
    "std_values = [st_1, st_2, st_3]\n",
    "eccentricities = [2.5, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431553e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0c570",
   "metadata": {},
   "source": [
    "## calculate cortical magnification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen = np.linspace(1, 11, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cmag_v1 = []\n",
    "all_cmag_v2 = []\n",
    "all_cmag_v3 = []\n",
    "all_cmag_v4 = []\n",
    "all_eccen_v1 = []\n",
    "all_eccen_v2 = []\n",
    "all_eccen_v3 = []\n",
    "all_eccen_v4 = []\n",
    "all_mask = ('variance_explained', 0.04, 1)\n",
    "\n",
    "for sid in sids_orig:\n",
    "    try:\n",
    "        sub = cc.cmag.load_subject(sid)\n",
    "        print(sid)\n",
    "        # Calculate cmag for the subject for V1\n",
    "        v1_mask = {'and': [('visual_area', 1), all_mask]}\n",
    "        eccen_v1, cmag_v1 = cc.cmag.ring_cmag(sub, eccen=None, mask=v1_mask)\n",
    "        all_eccen_v1.append(eccen_v1)\n",
    "        all_cmag_v1.append(cmag_v1)\n",
    "\n",
    "        # Calculate cmag for the subject for V2\n",
    "        v2_mask = {'and': [('visual_area', 2), all_mask]}\n",
    "        eccen_v2, cmag_v2 = cc.cmag.ring_cmag(sub, eccen=None, mask=v2_mask)\n",
    "        all_eccen_v2.append(eccen_v2)\n",
    "        all_cmag_v2.append(cmag_v2)\n",
    "\n",
    "        # Calculate cmag for the subject for V3\n",
    "        v3_mask = {'and': [('visual_area', 3), all_mask]}\n",
    "        eccen_v3, cmag_v3 = cc.cmag.ring_cmag(sub, eccen=None, mask=v3_mask)\n",
    "        all_eccen_v3.append(eccen_v3)\n",
    "        all_cmag_v3.append(cmag_v3)\n",
    "        \n",
    "        # Calculate cmag for the subject for V4\n",
    "        v4_mask = {'and': [('visual_area', 4), all_mask]}\n",
    "        eccen_v4, cmag_v4 = cc.cmag.ring_cmag(sub, eccen=None, mask=v4_mask)\n",
    "        all_eccen_v4.append(eccen_v4)\n",
    "        all_cmag_v4.append(cmag_v4)\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating cmag for subject {sid}: {e}\")\n",
    "\n",
    "all_flatcmag_v1 = np.concatenate(all_cmag_v1)\n",
    "all_flateccen_v1 = np.concatenate(all_eccen_v1)\n",
    "all_flatcmag_v2 = np.concatenate(all_cmag_v2)\n",
    "all_flateccen_v2 = np.concatenate(all_eccen_v2)\n",
    "all_flatcmag_v3 = np.concatenate(all_cmag_v3)\n",
    "all_flateccen_v3 = np.concatenate(all_eccen_v3)\n",
    "all_flatcmag_v4 = np.concatenate(all_cmag_v4)\n",
    "all_flateccen_v4 = np.concatenate(all_eccen_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = (all_flateccen_v1 < 11) & (all_flatcmag_v1 > 0) & (all_flateccen_v1 > 0.2)\n",
    "popt1 = cc.cmag.fit_cmag(all_flateccen_v1[ii], all_flatcmag_v1[ii],method='BFGS')\n",
    "popt1_3params = cc.cmag.fit_cmag(all_flateccen_v1[ii], all_flatcmag_v1[ii],p0=[17.3,0.75,2],method='BFGS')\n",
    "print(popt1,popt1_3params)\n",
    "plt.loglog(all_flateccen_v1,all_flatcmag_v1,'k.',alpha=0.01) \n",
    "plot_ecc = np.sort(all_flateccen_v1)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt1),'r-', zorder=10)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt1_3params),'c-', zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = (all_flateccen_v2 < 11) & (all_flatcmag_v2 > 0) & (all_flateccen_v2 > 0.2)\n",
    "popt2 = cc.cmag.fit_cmag(all_flateccen_v2[ii], all_flatcmag_v2[ii],method='BFGS')\n",
    "popt2_3params = cc.cmag.fit_cmag(all_flateccen_v2[ii], all_flatcmag_v2[ii],p0=[17.3,0.75,2],method='BFGS')\n",
    "print(popt2,popt2_3params)\n",
    "plt.loglog(all_flateccen_v2,all_flatcmag_v2,'k.',alpha=0.01) \n",
    "plot_ecc = np.sort(all_flateccen_v2)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt2),'r-', zorder=10)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt2_3params),'c-', zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = (all_flateccen_v3 < 11) & (all_flatcmag_v3 > 0) & (all_flateccen_v3 > 0.2)\n",
    "popt3 = cc.cmag.fit_cmag(all_flateccen_v3, all_flatcmag_v3,method='BFGS')\n",
    "popt3_3params = cc.cmag.fit_cmag(all_flateccen_v3[ii], all_flatcmag_v3[ii],p0=[17.3,0.75,2],method='BFGS')\n",
    "print(popt3,popt3_3params)\n",
    "plt.loglog(all_flateccen_v3,all_flatcmag_v3,'k.',alpha=0.01) \n",
    "plot_ecc = np.sort(all_flateccen_v3)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt3),'r-', zorder=10)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt3_3params),'c-', zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b526cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = (all_flateccen_v4 < 11) & (all_flatcmag_v4 > 0) & (all_flateccen_v4 > 0.2)\n",
    "popt4 = cc.cmag.fit_cmag(all_flateccen_v4[ii], all_flatcmag_v4[ii],method='BFGS')\n",
    "popt4_3params = cc.cmag.fit_cmag(all_flateccen_v4[ii], all_flatcmag_v4[ii],p0=[17.3,0.75,2],method='BFGS')\n",
    "print(popt4,popt4_3params)\n",
    "plt.loglog(all_flateccen_v4,all_flatcmag_v4,'k.',alpha=0.01) \n",
    "plot_ecc = np.sort(all_flateccen_v4)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt4),'r-', zorder=10)\n",
    "plt.loglog(plot_ecc, cc.cmag.HH91(plot_ecc, *popt4_3params),'c-', zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen = np.linspace(0.5, 11, 1000)\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen),'k-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt1),'b-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt2),'r-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt3),'c-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt4),'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen = np.linspace(0.5, 11, 1000)\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen),'k-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt1_3params),'b-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt2_3params),'r-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt3_3params),'c-')\n",
    "plt.loglog(eccen, cc.cmag.HH91(eccen, *popt4_3params),'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86901414",
   "metadata": {},
   "source": [
    "## add cortical magnification values to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnification_values is a dict containing cmag for each visual area\n",
    "magnification_values = cc.cmag.calculate_cortical_magnification(df)\n",
    "\n",
    "for mask_value in [1, 2, 3, 4]:\n",
    "    col_name = f'CorticalMagnification_{mask_value}'\n",
    "    df.loc[:, col_name] = magnification_values[mask_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_polar = df.groupby(['Eccen_X','Eccen_Y','ID'])[[\n",
    "    'CorticalMagnification_1',\n",
    "    'CorticalMagnification_2',\n",
    "    'CorticalMagnification_3',\n",
    "    'CorticalMagnification_4'\n",
    "]].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell = df_sub_polar\n",
    "# convert 2d cortical magnification into 1d visual magnification and change the names of columns accordingly\n",
    "for col in ['CorticalMagnification_1', 'CorticalMagnification_2', 'CorticalMagnification_3', 'CorticalMagnification_4']:\n",
    "    df_cell[col] = np.sqrt(1 / df_cell[col])\n",
    "df_cell.rename(columns={\n",
    "    'CorticalMagnification_1': 'VisualMagnification_1',\n",
    "    'CorticalMagnification_2': 'VisualMagnification_2',\n",
    "    'CorticalMagnification_3': 'VisualMagnification_3',\n",
    "    'CorticalMagnification_4': 'VisualMagnification_4'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce30d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the df\n",
    "df_cell.sort_values(by=['ID', 'Eccen_X','Eccen_Y'], inplace=True)\n",
    "# add the crowding values to the df\n",
    "mean_for_sub = mean_cd_polar['CrowdingDistance'].values\n",
    "df_cell['CrowdingDistance'] = mean_for_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56121db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df_sub_clean = df_cell.dropna(subset=['VisualMagnification_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17915816",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cd_polar[mean_cd_polar['ID']=='sub-wlsubj135']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_clean[df_sub_clean['ID']=='sub-wlsubj135']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413ab39",
   "metadata": {},
   "source": [
    "## plot visual magnification and crowding values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = np.unique(df_sub_clean['ID'])\n",
    "fig, axes = plt.subplots((len(sids)+1)//2,2, figsize=(8,16))\n",
    "x_ticks = [-10, -5, -2.5, 2.5, 5, 10]\n",
    "for sid, ax in zip(sids, axes.flat):\n",
    "    subject_data = df_sub_clean[(df_sub_clean['ID']==sid) & (df_sub_clean['Eccen_Y']==0)]\n",
    "    y1 = subject_data['VisualMagnification_1'].values\n",
    "    y2 = subject_data['VisualMagnification_2'].values\n",
    "    y3 = subject_data['VisualMagnification_3'].values\n",
    "    y4 = subject_data['VisualMagnification_4'].values\n",
    "    y5 = subject_data['CrowdingDistance'].values\n",
    "    \n",
    "    ax.plot(subject_data['Eccen_X'], y1, 'bo-')\n",
    "    ax.plot(subject_data['Eccen_X'], y2, 'ro-')\n",
    "    ax.plot(subject_data['Eccen_X'], y3, 'co-')\n",
    "    ax.plot(subject_data['Eccen_X'], y4, 'go-')\n",
    "    ax.plot(subject_data['Eccen_X'], y5, 'ko-')\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_title(sid)\n",
    "    ax.set_xlabel(\"Horizontal meridian\")\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots((len(sids)+1)//2,2, figsize=(8,16))\n",
    "x_ticks = [-10, -5, -2.5, 2.5, 5, 10]\n",
    "\n",
    "for sid, ax in zip(sids, axes.flat):\n",
    "    subject_data = df_sub_clean[(df_sub_clean['ID']==sid) & (df_sub_clean['Eccen_X']==0)]\n",
    "    y1 = subject_data['VisualMagnification_1'].values\n",
    "    y2 = subject_data['VisualMagnification_2'].values\n",
    "    y3 = subject_data['VisualMagnification_3'].values\n",
    "    y4 = subject_data['VisualMagnification_4'].values\n",
    "    y5 = subject_data['CrowdingDistance'].values\n",
    "    \n",
    "    ax.plot(subject_data['Eccen_Y'], y1, 'bo-')\n",
    "    ax.plot(subject_data['Eccen_Y'], y2, 'ro-')\n",
    "    ax.plot(subject_data['Eccen_Y'], y3, 'co-')\n",
    "    ax.plot(subject_data['Eccen_Y'], y4, 'go-')\n",
    "    ax.plot(subject_data['Eccen_Y'], y5, 'ko-')\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_title(sid)\n",
    "    ax.set_xlabel(\"Vertical meridian\")\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699afe62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cf84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "688502d1",
   "metadata": {},
   "source": [
    "## use cortical magnification to predict crowding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = np.unique(df_sub_clean['ID'])\n",
    "\n",
    "fig, axes = plt.subplots((len(sids)+1)//2,2, figsize=(8,16))\n",
    "\n",
    "for sid, ax in zip(sids, axes.flat):\n",
    "    subject_data = df_sub_clean[df_sub_clean['ID']==sid]\n",
    "    y1 = subject_data['VisualMagnification_1'].values\n",
    "    y2 = subject_data['VisualMagnification_2'].values\n",
    "    y3 = subject_data['VisualMagnification_3'].values\n",
    "    y4 = subject_data['VisualMagnification_4'].values\n",
    "    y5 = subject_data['CrowdingDistance'].values\n",
    "\n",
    "    rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "    rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "    rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "    rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "\n",
    "    ax.plot(y1, y5, 'ro-')\n",
    "    ax.plot(y2, y5, 'go-')\n",
    "    ax.plot(y3, y5, 'bo-')\n",
    "    ax.plot(y4, y5, 'ko-')\n",
    "    ax.scatter([0],[0],color = '0.5')\n",
    "#ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea334ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_sub_clean.groupby('ID')\n",
    "subject_results = {}\n",
    "\n",
    "# Iterate over each subject to do the regression\n",
    "for subject, subject_data in grouped:\n",
    "    y1 = subject_data['VisualMagnification_1'].values\n",
    "    y2 = subject_data['VisualMagnification_2'].values\n",
    "    y3 = subject_data['VisualMagnification_3'].values\n",
    "    y4 = subject_data['VisualMagnification_4'].values\n",
    "    y5 = subject_data['CrowdingDistance'].values\n",
    "\n",
    "    \n",
    "    rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "    rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "    rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "    rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "    \n",
    "    rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "    best_fit_index = np.argmin(rss_values)\n",
    "    best_fit = ['y1','y2','y3','y4'][best_fit_index]\n",
    "    \n",
    "    # create a dict to store values\n",
    "    subject_results[subject] = {\n",
    "        'rss_y1': rss_y1, 'coef_y1': coef_y1,\n",
    "        'rss_y2': rss_y2, 'coef_y2': coef_y2,\n",
    "        'rss_y3': rss_y3, 'coef_y3': coef_y3,\n",
    "        'rss_y4': rss_y4, 'coef_y4': coef_y4,\n",
    "        'best_fit': best_fit\n",
    "    }\n",
    "\n",
    "for subject, results in subject_results.items():\n",
    "    print(f\"Results for {subject}:\")\n",
    "    print(f\"RSS for y1: {results['rss_y1']}, Coefficient: {results['coef_y1']}\")\n",
    "    print(f\"RSS for y2: {results['rss_y2']}, Coefficient: {results['coef_y2']}\")\n",
    "    print(f\"RSS for y3: {results['rss_y3']}, Coefficient: {results['coef_y3']}\")\n",
    "    print(f\"RSS for y4: {results['rss_y4']}, Coefficient: {results['coef_y4']}\")\n",
    "    print(f\"The best fit is: {results['best_fit']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60742959",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'rss_y1'\n",
    "data = [[subdata[key] for subdata in subject_results.values()]\n",
    "        for key in ['rss_y1','rss_y2','rss_y3','rss_y4']]\n",
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_y1_values = []\n",
    "rss_y2_values = []\n",
    "rss_y3_values = []\n",
    "rss_y4_values = []\n",
    "\n",
    "for subject, values in subject_results.items():\n",
    "    rss_y1_values.append(values['rss_y1'])\n",
    "    rss_y2_values.append(values['rss_y2'])\n",
    "    rss_y3_values.append(values['rss_y3'])\n",
    "    rss_y4_values.append(values['rss_y4'])\n",
    "\n",
    "mean_rss_y1 = np.mean(rss_y1_values)\n",
    "mean_rss_y2 = np.mean(rss_y2_values)\n",
    "mean_rss_y3 = np.mean(rss_y3_values)\n",
    "mean_rss_y4 = np.mean(rss_y4_values)\n",
    "\n",
    "std_rss_y1 = np.std(rss_y1_values)\n",
    "std_rss_y2 = np.std(rss_y2_values)\n",
    "std_rss_y3 = np.std(rss_y3_values)\n",
    "std_rss_y4 = np.std(rss_y4_values)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean RSS for y1: {mean_rss_y1}, Standard Deviation: {std_rss_y1}\")\n",
    "print(f\"Mean RSS for y2: {mean_rss_y2}, Standard Deviation: {std_rss_y2}\")\n",
    "print(f\"Mean RSS for y3: {mean_rss_y3}, Standard Deviation: {std_rss_y3}\")\n",
    "print(f\"Mean RSS for y4: {mean_rss_y4}, Standard Deviation: {std_rss_y4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_y1, mean_rss_y2, mean_rss_y3, mean_rss_y4]\n",
    "std_rss = [std_rss_y1, std_rss_y2, std_rss_y3, std_rss_y4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=std_rss/np.sqrt(len(sids)))\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('RadialEccentricity')[[\n",
    "    'CorticalMagnification_1',\n",
    "    'CorticalMagnification_2',\n",
    "    'CorticalMagnification_3',\n",
    "    'CorticalMagnification_4'\n",
    "]].mean().reset_index()\n",
    "\n",
    "# make it 1d visual magnification\n",
    "for col in ['CorticalMagnification_1', 'CorticalMagnification_2', 'CorticalMagnification_3', 'CorticalMagnification_4']:\n",
    "    grouped_df[col] = np.sqrt(1 / grouped_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944db937",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['CrowdingDistance'] = mean_values\n",
    "grouped_df.rename(columns={\n",
    "    'CorticalMagnification_1': 'VisualMagnification_1',\n",
    "    'CorticalMagnification_2': 'VisualMagnification_2',\n",
    "    'CorticalMagnification_3': 'VisualMagnification_3',\n",
    "    'CorticalMagnification_4': 'VisualMagnification_4'\n",
    "}, inplace=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5412069",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for col in ['VisualMagnification_1', 'VisualMagnification_2', 'VisualMagnification_3', 'VisualMagnification_4']:\n",
    "    plt.scatter(grouped_df['RadialEccentricity'], grouped_df[col], marker='o', linestyle='-', label=col)\n",
    "plt.scatter(eccentricities, mean_values, color='cyan', label = 'crowding distance')\n",
    "plt.xlabel('Radial Eccentricity')\n",
    "plt.ylabel('Visual Magnification')\n",
    "plt.title('Visual Magnification vs Radial Eccentricity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = grouped_df['VisualMagnification_1'].values\n",
    "y2 = grouped_df['VisualMagnification_2'].values\n",
    "y3 = grouped_df['VisualMagnification_3'].values\n",
    "y4 = grouped_df['VisualMagnification_4'].values\n",
    "y5 = grouped_df['CrowdingDistance'].values\n",
    "\n",
    "rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "\n",
    "rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "best_fit_index = np.argmin(rss_values)\n",
    "best_fit = ['y1', 'y2', 'y3', 'y4'][best_fit_index]\n",
    "\n",
    "print(f\"RSS for y1: {rss_y1}, Coefficient: {coef_y1}\")\n",
    "print(f\"RSS for y2: {rss_y2}, Coefficient: {coef_y2}\")\n",
    "print(f\"RSS for y3: {rss_y3}, Coefficient: {coef_y3}\")\n",
    "print(f\"RSS for y4: {rss_y4}, Coefficient: {coef_y4}\")\n",
    "print(f\"The best fit is: {best_fit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44012683",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for scale, col in zip([coef_y1,coef_y2,coef_y3,coef_y4],['VisualMagnification_1', 'VisualMagnification_2', 'VisualMagnification_3', 'VisualMagnification_4']):\n",
    "    plt.scatter(grouped_df['RadialEccentricity'], scale * grouped_df[col], marker='o', linestyle='-', label=col)\n",
    "plt.scatter(eccentricities, mean_values, color='cyan', label = 'crowding distance')\n",
    "plt.xlabel('Radial Eccentricity')\n",
    "plt.ylabel('Scaled Visual Magnification')\n",
    "plt.title('Scaled Visual Magnification vs Radial Eccentricity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddea751",
   "metadata": {},
   "source": [
    "## Testing alternative fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffe6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HH91_vmag(ecc, a=17.3, b=0.75, c=2):\n",
    "    return ((np.asarray(ecc) + b) / a)**c\n",
    "\n",
    "def fit_vmag(ecc, cmag, params0=(17.3, 0.75), method=None):\n",
    "    from scipy.optimize import minimize\n",
    "    ecc = np.asarray(ecc, dtype=np.float64)\n",
    "    cmag = np.asarray(cmag, dtype=np.float64)\n",
    "    vmag = 1/cmag\n",
    "    def loss_vmag(params):\n",
    "        params = list(params)\n",
    "        params[1] = params[1]**2\n",
    "        pred = HH91_vmag(ecc, *params)\n",
    "        error = (pred - vmag)\n",
    "        return np.sum(error**2)\n",
    "    params0 = list(params0)\n",
    "    params0[1] = np.sqrt(params0[1])\n",
    "    r = minimize(loss_vmag, params0, method=method)\n",
    "    r.x[1] = r.x[1]**2\n",
    "    r.coords = np.array([ecc,cmag])\n",
    "    return r\n",
    "\n",
    "def trimsame(x, *args):\n",
    "    x = np.asarray(x)\n",
    "    idx0 = np.where(x != x[0])[0][0] - 1\n",
    "    idxn = np.where(~np.isclose(x, x[-1]))[0][-1] + 1\n",
    "    return (x[idx0:idxn],) + tuple(arg[idx0:idxn] for arg in args)\n",
    "\n",
    "def fit_subject_vmag(sid,\n",
    "                     params0=(17.3, 0.75),\n",
    "                     method=None,\n",
    "                     allmask={'and': [('variance_explained', 0.04, 1),\n",
    "                                      ('eccentricity', 0, 12)]}):\n",
    "    sub = cc.cmag.load_subject(sid)\n",
    "    cmags = []\n",
    "    eccens = []\n",
    "    results = []\n",
    "    for k in range(1,5):\n",
    "        mask = {'and': [('visual_area', k), allmask]}\n",
    "        eccen, cmag = cc.cmag.ring_cmag(sub, mask=mask)\n",
    "        (cmag, eccen) = trimsame(cmag, eccen)\n",
    "        eccens.append(eccen)\n",
    "        cmags.append(cmag)\n",
    "        results.append(fit_vmag(eccen, cmag, params0, method=method))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = fit_subject_vmag('sub-wlsubj150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig,ax) = plt.subplots(1,1, figsize=(5,3), dpi=288)\n",
    "\n",
    "eccrng = np.linspace(0, 11, 500)\n",
    "for (clr,r) in zip(['r','g','b','k'], rs):\n",
    "    #if clr != 'r': continue\n",
    "    (ecc,cmag) = r.coords\n",
    "    if len(r.x) == 3:\n",
    "        (a,b,c) = r.x\n",
    "    else:\n",
    "        (a,b) = r.x\n",
    "        c = 2\n",
    "    ax.loglog(ecc, cmag, clr + '.')\n",
    "    ax.loglog(eccrng, 1/HH91_vmag(eccrng, a, b, c), clr + '-')\n",
    "ax.set_ylim([0.5,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b663011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmag_basics(sid, h, label):\n",
    "    sub = cc.cmag.load_subject(sid)\n",
    "    hem = sub.hemis[h]\n",
    "    mask_nor2 = {\n",
    "        'and': [\n",
    "            ('eccentricity', 0, 12),\n",
    "            ('visual_area', label)]}\n",
    "    mask_r2 = {'and': mask_nor2['and'] + [('variance_explained', 0.04, 1)]}\n",
    "    rdat = ny.retinotopy_data(hem)\n",
    "    mask_ii = hem.mask(mask_r2)\n",
    "    ecc = rdat['eccentricity'][mask_ii]\n",
    "    srf = hem.prop('midgray_surface_area')\n",
    "    totarea = np.sum(srf[hem.mask(mask_nor2)])\n",
    "    srf = srf[mask_ii]\n",
    "    ii = np.argsort(ecc)\n",
    "    ecc = ecc[ii]\n",
    "    srf = srf[ii] * totarea / np.sum(srf)\n",
    "    return (ecc, srf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b45f72",
   "metadata": {},
   "source": [
    "$$ c_1 = \\sqrt{\\frac{a_M}{\\pi \\left(\\log\\left(\\frac{c_2 + M}{c_2}\\right) - \\frac{M}{c_2 + M}\\right)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8127d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HH91_integral(x, a=17.3, b=0.75):\n",
    "    xb = x + b\n",
    "    return a**2 * np.pi * (np.log(xb / b) - x / xb)\n",
    "\n",
    "def HH91_c1(totalarea, maxecc, b=0.75):\n",
    "    mb = maxecc + b\n",
    "    c1 = np.sqrt(totalarea / np.pi / (np.log(mb / b) - maxecc/mb))\n",
    "    \n",
    "def fit_cumarea_data(ecc, srf, params0=(17.3, 0.75), method=None):\n",
    "    from scipy.optimize import minimize\n",
    "    ecc = np.asarray(ecc, dtype=np.float64)\n",
    "    srf = np.asarray(srf, dtype=np.float64)\n",
    "    ii = np.argsort(ecc)\n",
    "    ecc = ecc[ii]\n",
    "    srf = srf[ii]\n",
    "    cumsrf = np.cumsum(srf)\n",
    "    def loss_vmag(params):\n",
    "        params = list(params)\n",
    "        params[1] = params[1]**2\n",
    "        pred = HH91_integral(ecc, *params)\n",
    "        error = (pred - cumsrf)\n",
    "        return np.mean(error**2)\n",
    "    params0 = list(params0)\n",
    "    params0[1] = np.sqrt(params0[1])\n",
    "    r = minimize(loss_vmag, params0, method=method)\n",
    "    r.x[0] = abs(r.x[0])\n",
    "    r.x[1] = r.x[1]**2\n",
    "    r.coords = np.array([ecc,srf])\n",
    "    return r\n",
    "\n",
    "def fit_cumarea(sid, h, label):\n",
    "    (ecc,srf) = cmag_basics(sid, h, label)\n",
    "    if len(ecc) == 0:\n",
    "        raise RuntimeError(f\"no data found for {sid}:{h}:{label}\")\n",
    "    r = fit_cumarea_data(ecc, srf)\n",
    "    r.coords = np.array([ecc, srf])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 'sub-wlsubj136'\n",
    "\n",
    "(fig,axs) = plt.subplots(1,2, figsize=(7,3), dpi=288)\n",
    "eccrng = np.linspace(0, 11, 500)\n",
    "\n",
    "for (h,ax) in zip(['lh','rh'], axs):\n",
    "    for (clr, lbl) in zip(['r','g','b','k'], [1,2,3,4]):\n",
    "        if lbl != 4:\n",
    "            continue\n",
    "        r = fit_cumarea(sid, h, lbl)\n",
    "        (ecc,srf) = r.coords\n",
    "        (a,b) = r.x\n",
    "        print(f\"{sid:<13s} {h:2s} V{lbl}: a={a:5.2f}, b={b:5.3f}\")\n",
    "        ax.plot(ecc, np.cumsum(srf), clr + '.', lw=0.5, alpha=0.05)\n",
    "        ax.plot(ecc, HH91_integral(ecc, a, b), clr + '-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a52e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dict(sid=[], h=[], label=[], a=[], b=[], loss=[])\n",
    "for sid in sids_orig:\n",
    "    print(sid)\n",
    "    for h in ['lh','rh']:\n",
    "        for lbl in [1,2,3,4]:\n",
    "            try:\n",
    "                r = fit_cumarea(sid, h, lbl)\n",
    "            except Exception as e:\n",
    "                print(f\"  - Skipping: {type(e)}\")\n",
    "                continue\n",
    "            df['sid'].append(sid)\n",
    "            df['h'].append(h)\n",
    "            df['label'].append(lbl)\n",
    "            df['a'].append(r.x[0])\n",
    "            df['b'].append(r.x[1])\n",
    "            df['loss'].append(r.fun)\n",
    "HH91_params = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH91_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        q = df2['a'].values\n",
    "        ax.hist(q)\n",
    "        ax.set_xlim([0,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        q = df2['b'].values\n",
    "        ax.hist(q)\n",
    "        #ax.set_xlim([0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(0,0,1,1,0.1,0.1)\n",
    "eccrng = np.linspace(0.5, 11, 500)\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        for (a,b) in zip(df2['a'].values, df2['b'].values):\n",
    "            ax.loglog(eccrng, (a / (b + eccrng))**2, 'k-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a70c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH91_params[(HH91_params['label']==4) & (HH91_params['h']=='rh')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a9442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(1,2, figsize=(4,1.5), dpi=288, sharex=True, sharey=True)\n",
    "eccrng = np.linspace(0.5, 12, 500)\n",
    "for (lbl,clr) in zip([1,2,3,4], ['r','g','b','k']):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axs):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        a = df2['a'].values\n",
    "        b = df2['b'].values\n",
    "        m = (a[:,None] / (b[:,None] + eccrng[None,:]))**2\n",
    "        mu = np.mean(m, axis=0)\n",
    "        sd = np.std(m, axis=0)\n",
    "        ax.fill_between(eccrng, mu - sd, mu + sd, edgecolor=None, facecolor=clr, alpha=0.2, zorder=-1)\n",
    "        ax.loglog(eccrng, mu, clr+'-', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c59fbd",
   "metadata": {},
   "source": [
    "## rerun analysis using the new fitting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at horizontal meridian for tmp_hor\n",
    "# h is assigned 'lh' for x values larger than 0 and 'rh' otherwise\n",
    "tmp_hor = mean_cd_polar[mean_cd_polar['Eccen_Y']==0]\n",
    "x = tmp_hor['Eccen_X'].values\n",
    "# for calculating cmag, have a col for abs(x)\n",
    "tmp_hor['eccen'] = np.abs(x)\n",
    "tmp_hor['h'] = np.where(x > 0, 'lh', 'rh')\n",
    "tmp_hor.rename(columns={'CrowdingDistance': 'CrowdingDistance_HM'}, inplace=True)\n",
    "tmp_hor.rename(columns={'ID': 'sid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at vertical meridian for tmp_ver\n",
    "tmp_ver = mean_cd_polar[mean_cd_polar['Eccen_X']==0]\n",
    "y = tmp_ver['Eccen_Y'].values\n",
    "tmp_ver['eccen'] = np.abs(y)\n",
    "\n",
    "# Create the 'CrowdingDistance_UVM' and 'CrowdingDistance_LVM' columns\n",
    "tmp_ver['CrowdingDistance_UVM'] = np.where(y > 0, np.nan, tmp_ver['CrowdingDistance'])\n",
    "tmp_ver['CrowdingDistance_LVM'] = np.where(y > 0, tmp_ver['CrowdingDistance'], np.nan)\n",
    "tmp_ver.rename(columns={'ID': 'sid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be74eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create duplicate rows with 'h' column\n",
    "tmp_ver_lh = tmp_ver.copy()\n",
    "tmp_ver_lh['h'] = 'lh'\n",
    "tmp_ver_rh = tmp_ver.copy()\n",
    "tmp_ver_rh['h'] = 'rh'\n",
    "\n",
    "# Combine the two DataFrames, so now tmp_ver has two rows with same data except one row lh and one row rh\n",
    "tmp_ver = pd.concat([tmp_ver_lh, tmp_ver_rh], ignore_index=True)\n",
    "tmp_ver = tmp_ver.drop(columns=['CrowdingDistance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee551629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params.copy()\n",
    "\n",
    "# will have three rows for diff ecc, but the same a,b params\n",
    "df = df.merge(\n",
    "    pd.DataFrame(dict(eccen=[2.5, 5.0, 10.0])),\n",
    "    how='cross')\n",
    "\n",
    "a = df['a']\n",
    "b = df['b']\n",
    "ecc = df['eccen']\n",
    "# calculate cmag based on a,b params from HH91_params\n",
    "df['cmag_fit'] = (a / (ecc + b))**2\n",
    "# add 1d visual magnification\n",
    "df['1d_vmag_fit'] = np.sqrt(1/ (a / (ecc + b))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add CrowdingDistance_HM to df by merging it with tmp_hor\n",
    "# _full versions have x, y positions too\n",
    "df_HM_full = df.merge(tmp_hor, on=['sid', 'eccen','h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce892b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM = df_HM_full.drop(columns=['Eccen_X', 'Eccen_Y']) # 15 subs, 2 hemisphere * 3 ecc * 4 visual area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM_full = df.merge(tmp_ver, on=['sid', 'eccen','h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e809e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM = df_VM_full.drop(columns=['Eccen_X', 'Eccen_Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9221f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average of lh and rh cmag fits\n",
    "df_VM = df_VM.groupby(\n",
    "    ['sid', 'label', 'eccen'], as_index=False\n",
    ").agg({\n",
    "    'a': 'mean',\n",
    "    'b': 'mean',\n",
    "    'loss': 'mean',\n",
    "    'cmag_fit': 'mean',\n",
    "    '1d_vmag_fit': 'mean',\n",
    "    'CrowdingDistance_UVM': np.nanmean, \n",
    "    'CrowdingDistance_LVM': np.nanmean, \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804887ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 4\n",
    "h = 'rh'\n",
    "\n",
    "fig, axs = plt.subplots(1,1, figsize=(4,2), dpi=288)\n",
    "df_sub = df_HM[(df_HM['label']==label) & (df_HM['h']==h)]\n",
    "loss = df_sub['loss']\n",
    "vmag = df_sub['1d_vmag_fit']\n",
    "cd = df_sub['CrowdingDistance_HM']\n",
    "eccen = df_sub['eccen']\n",
    "\n",
    "for (e,color) in zip([2.5,5,10],'rgb'):\n",
    "    ii = (eccen==e)\n",
    "    axs.scatter(vmag[ii], cd[ii], s=4*(2-loss[ii]/2000), c=color)\n",
    "axs.plot([0,1],[0,1],'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b936de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots based on HM measurements\n",
    "fig, axs = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(0,0,1,1,0.1,0.3)\n",
    "\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df_HM[df_HM['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        loss = df2['loss']\n",
    "        vmag = df2['1d_vmag_fit']\n",
    "        cd = df2['CrowdingDistance_HM']\n",
    "        eccen = df2['eccen']\n",
    "        for (e,color) in zip([2.5,5,10],'rgb'):\n",
    "            ii = (eccen==e)\n",
    "            ax.scatter(vmag[ii], cd[ii], s=4*(2-loss[ii]/2000), c=color)\n",
    "            ax.plot([0,2],[0,2],'k-')\n",
    "            ax.set_title(f'{h} Label={lbl}', fontsize=6)\n",
    "            \n",
    "fig.text(0.5, -0.1, '1D Vmag Fit', ha='center', fontsize=8)\n",
    "fig.text(-0.1, 0.5, 'Crowding Distance', va='center', rotation='vertical', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45984446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {1:[],2:[],3:[],4:[]}\n",
    "for subject, subject_data in df_HM.groupby(['sid']):\n",
    "    for h in ['lh','rh']:\n",
    "        ssdf = subject_data[subject_data['h'] == h]\n",
    "        for lbl in [1,2,3,4]:\n",
    "            sssdf = ssdf[ssdf['label'] == lbl]\n",
    "            x = sssdf['1d_vmag_fit'].values\n",
    "            y = sssdf['CrowdingDistance_HM'].values\n",
    "            (rss,coef) = cc.regression.fit_and_evaluate(x, y)\n",
    "            res[lbl].append(rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a43647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = df_HM.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b']>5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "    for h in ['lh', 'rh']:\n",
    "        hemisphere_data = subject_data[subject_data['h'] == h]\n",
    "        \n",
    "        y1 = hemisphere_data[hemisphere_data['label'] == 1]['1d_vmag_fit'].values\n",
    "        y2 = hemisphere_data[hemisphere_data['label'] == 2]['1d_vmag_fit'].values\n",
    "        y3 = hemisphere_data[hemisphere_data['label'] == 3]['1d_vmag_fit'].values\n",
    "        y4 = hemisphere_data[hemisphere_data['label'] == 4]['1d_vmag_fit'].values\n",
    "        # since cd are the same for label 1,2,3,4\n",
    "        y5 = hemisphere_data[hemisphere_data['label'] == 1]['CrowdingDistance_HM'].values\n",
    "        \n",
    "        rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "        rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "        rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "        rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "        \n",
    "        rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "        best_fit_index = np.argmin(rss_values)\n",
    "        best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "        \n",
    "        subject_results[subject][h] = {\n",
    "            'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "            'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "            'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "            'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "            'best_fit': best_fit\n",
    "        }\n",
    "\n",
    "for subject, hemispheres in subject_results.items():\n",
    "    print(f\"Results for {subject}:\")\n",
    "    for h, results in hemispheres.items():\n",
    "        print(f\"  Hemisphere: {h}\")\n",
    "        print(f\"    RSS for label 1: {results['rss_label_1']}, Coefficient: {results['coef_label_1']}\")\n",
    "        print(f\"    RSS for label 2: {results['rss_label_2']}, Coefficient: {results['coef_label_2']}\")\n",
    "        print(f\"    RSS for label 3: {results['rss_label_3']}, Coefficient: {results['coef_label_3']}\")\n",
    "        print(f\"    RSS for label 4: {results['rss_label_4']}, Coefficient: {results['coef_label_4']}\")\n",
    "        print(f\"    The best fit is: {results['best_fit']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50834602",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_keys = ['rss_label_1', 'rss_label_2', 'rss_label_3', 'rss_label_4']\n",
    "\n",
    "data = [[\n",
    "    hemispheres[h][key]  \n",
    "    for subject, hemispheres in subject_results.items()\n",
    "    for h in ['lh', 'rh'] \n",
    "] for key in rss_keys]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, label=['Label 1', 'Label 2', 'Label 3', 'Label 4'])\n",
    "plt.xlabel('RSS Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of RSS Values for Each Label')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d15397",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, hemispheres in subject_results.items():\n",
    "    for h in ['lh', 'rh']:\n",
    "        rss_label_1_values.append(hemispheres[h]['rss_label_1'])\n",
    "        rss_label_2_values.append(hemispheres[h]['rss_label_2'])\n",
    "        rss_label_3_values.append(hemispheres[h]['rss_label_3'])\n",
    "        rss_label_4_values.append(hemispheres[h]['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c26b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_label_1, mean_rss_label_2, mean_rss_label_3, mean_rss_label_4]\n",
    "std_rss = [std_rss_label_1, std_rss_label_2, std_rss_label_3, std_rss_label_4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# standard error of the mean (SEM)\n",
    "n_subjects = len(subject_results) * 2  # for both hemispheres\n",
    "sem_rss = np.array(std_rss) / np.sqrt(n_subjects)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=sem_rss)\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce4251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:np.mean(v) for (k,v) in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddd8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:np.std(v) for (k,v) in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935ded5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dce51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
