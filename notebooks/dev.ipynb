{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7af612-e71d-4dfc-a477-088a7f79c1b6",
   "metadata": {},
   "source": [
    "# Development Notebook for Cortical Crowding Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f43079",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8a076-56ce-45b0-950d-b70220b4d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuropythy as ny\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize\n",
    "from scipy.stats import gmean\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ad992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpl.rcParams['font.family'] = 'Arial'\n",
    "mpl.rcParams['font.family'] = 'HelveticaNeue'\n",
    "mpl.rcParams['font.size'] = 10\n",
    "mpl.rcParams['font.weight'] = 'light'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "mpl.rcParams['figure.dpi'] = 576  # 72*8\n",
    "mpl.rcParams['hatch.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328aa4ac-48d3-46ba-8748-e8c287d3edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to be able to load in libraries that are in this repository's src directory,\n",
    "# so we add src to the system path:\n",
    "try:\n",
    "    import corticalcrodwing as cc\n",
    "except ModuleNotFoundError:\n",
    "    # This probably happens because the corticalcrowding library hasn't been\n",
    "    # installed yet; we can add the src directory to the path to work around\n",
    "    # this here.\n",
    "    sys.path.append('../src')\n",
    "    # Now we can import corticalcrowding from the src directory:\n",
    "    import corticalcrowding as cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085564f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The root path where data is stored:\n",
    "data_path = Path('/data/crowding')\n",
    "\n",
    "# The crowding data CSV file:\n",
    "crowding_data_filename = data_path / 'crowding_data_withID.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fe894",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 576  # 72*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb702d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of subjects:\n",
    "sids_NYU = [\n",
    "    'sub-wlsubj070',\n",
    "    'sub-wlsubj114',\n",
    "    'sub-wlsubj121',\n",
    "    'sub-wlsubj135']\n",
    "\n",
    "# 36 is used\n",
    "sids_NEI = [ \n",
    "    'sub-wlsubj119',\n",
    "    'sub-wlsubj127',\n",
    "    'sub-wlsubj136',\n",
    "    'sub-wlsubj137',\n",
    "    'sub-wlsubj143',\n",
    "    'sub-wlsubj144',\n",
    "    'sub-wlsubj145',\n",
    "    'sub-wlsubj146',\n",
    "    'sub-wlsubj147',\n",
    "    'sub-wlsubj148',\n",
    "    'sub-wlsubj149',\n",
    "    'sub-wlsubj150',\n",
    "    'sub-wlsubj151',\n",
    "    'sub-wlsubj152',\n",
    "    'sub-wlsubj153',\n",
    "    'sub-wlsubj154',\n",
    "    'sub-wlsubj155',\n",
    "    'sub-wlsubj156',\n",
    "    'sub-wlsubj157',\n",
    "    'sub-wlsubj158',\n",
    "    'sub-wlsubj159',\n",
    "    'sub-wlsubj160',\n",
    "    'sub-wlsubj161',\n",
    "    'sub-wlsubj162',\n",
    "    'sub-wlsubj163',\n",
    "    'sub-wlsubj164',\n",
    "    'sub-wlsubj165',\n",
    "    'sub-wlsubj166',\n",
    "    'sub-wlsubj167',\n",
    "    'sub-wlsubj168',\n",
    "    'sub-wlsubj170',\n",
    "    'sub-wlsubj171',\n",
    "    'sub-wlsubj172',\n",
    "    'sub-wlsubj173',\n",
    "    'sub-wlsubj174',\n",
    "    'sub-wlsubj175',\n",
    "    'sub-wlsubj176']\n",
    "\n",
    "sids_orig = sids_NYU + sids_NEI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac4774",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowding_data = pd.read_csv(crowding_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c2085",
   "metadata": {},
   "source": [
    "## Crowding Distance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 sessions, so now each sub have 12 cd (match with polar angle) instead of 24, 3 eccentric * 2session * 4 meridian\n",
    "mean_cd_polar = (\n",
    "    crowding_data\n",
    "    .groupby(['ID','Eccen_X','Eccen_Y'])\n",
    "    ['CrowdingDistance']\n",
    "    .apply(gmean)\n",
    "    .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each subject has 1 cd value at each eccentricity\n",
    "mean_cd = (\n",
    "    crowding_data\n",
    "    .groupby(['ID','RadialEccentricity'])\n",
    "    ['CrowdingDistance']\n",
    "    .apply(gmean)\n",
    "    .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the bouma factor\n",
    "[val / div for val, div in zip(mean_values, eccentricities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_list = crowding_data['CrowdingDistance'].tolist()\n",
    "mean_cd_list = mean_cd['CrowdingDistance'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 dfs based on the eccentricities in the dataframe:\n",
    "crowding_eccens = np.unique(crowding_data['RadialEccentricity'])\n",
    "assert len(crowding_eccens) == 3\n",
    "(ecc_1, ecc_2, ecc_3) = crowding_eccens\n",
    "\n",
    "mean_1 = mean_cd[mean_cd['RadialEccentricity'] == ecc_1]\n",
    "n_1 = len(mean_1)\n",
    "m_1 = mean_1['CrowdingDistance'].mean()\n",
    "st_1 = mean_1['CrowdingDistance'].std()\n",
    "\n",
    "mean_2 = mean_cd[mean_cd['RadialEccentricity'] == ecc_2]\n",
    "n_2 = len(mean_2)\n",
    "m_2 = mean_2['CrowdingDistance'].mean()\n",
    "st_2 = mean_2['CrowdingDistance'].std()\n",
    "\n",
    "mean_3 = mean_cd[mean_cd['RadialEccentricity'] == ecc_3]\n",
    "n_3 = len(mean_3)\n",
    "m_3 = mean_3['CrowdingDistance'].mean()\n",
    "st_3 = mean_3['CrowdingDistance'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecc = crowding_data['RadialEccentricity'].tolist()\n",
    "mean_x_ecc = mean_cd['RadialEccentricity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The crowding distance function in terms of eccentricity function described\n",
    "# by Kurzawski et al. (2023):\n",
    "Kurzawski2023_cd = cc.crowding.Kurzawski2023_cd\n",
    "# We wrap this with a log10 so that we can fit using log errors.\n",
    "def log_Kurzawski2023_cd(x, b):\n",
    "    return np.log10(Kurzawski2023_cd(x, b))\n",
    "\n",
    "# Fit the b parameter using this function by minimizing log error.\n",
    "b, _ = curve_fit(log_Kurzawski2023_cd, x_ecc, np.log10(cd_list), p0=0.15)\n",
    "b = b[0]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21810d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = [m_1, m_2, m_3]\n",
    "std_values = [st_1, st_2, st_3]\n",
    "sem_values = np.array([st_1, st_2, st_3]) / np.sqrt([n_1, n_2, n_3])\n",
    "eccentricities = [ecc_1, ecc_2, ecc_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431553e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(eccentricities, mean_values, sem_values, fmt='o')\n",
    "plt.gca().set_xlabel('Eccentricity [deg]')\n",
    "plt.gca().set_ylabel(r'Crowding Distance $(\\mu \\pm SEM)$ [deg]')\n",
    "x = np.linspace(0,11,500)\n",
    "y = Kurzawski2023_cd(x, b)\n",
    "plt.gca().plot(x, y, 'k:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd336fc2",
   "metadata": {},
   "source": [
    "### bootstrap on crowding distance fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstrap_samples = 1000\n",
    "x = np.linspace(0.5,11,1000)\n",
    "eccentricities = [2.5, 5, 10]\n",
    "\n",
    "# sid_df.shape=(480,)\n",
    "sid_df = crowding_data['ID'].values\n",
    "x_ecc = np.array(x_ecc)\n",
    "cd = np.array(cd_list)\n",
    "\n",
    "def bootstrap_fit(sids, xdata, ydata, x):\n",
    "    # unique_sids : 20 numbers\n",
    "    unique_sids = np.unique(sids)\n",
    "    bootstrapped_parameters = []\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        # each bootstrap, sample 20 subjects with replacement\n",
    "        indices = np.random.choice(unique_sids, size=len(unique_sids), replace=True)\n",
    "        indices = [np.where(sids == sid)[0] for sid in indices]\n",
    "        indices = [k for ak in indices for k in ak]\n",
    "        # 20 by 24 = 480, 480 x values and y values each\n",
    "        x_boot = xdata[indices]\n",
    "        y_boot = ydata[indices]\n",
    "        # Fit the curve to the bootstrapped sample\n",
    "        b, _ = curve_fit(log_Kurzawski2023_cd, x_boot, np.log10(y_boot), p0=0.15)\n",
    "        y = (0.43 + x + 0.06*(x**2)) * b\n",
    "        bootstrapped_parameters.append(y) \n",
    "    return bootstrapped_parameters\n",
    "\n",
    "bootstrapped = bootstrap_fit(sid_df, x_ecc, cd, x)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval_cd = np.percentile(bootstrapped, [2.5, 97.5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4264cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 11, 1000)\n",
    "plt.figure(figsize=(3.5, 3))\n",
    "\n",
    "# Fitted value without bootstrap\n",
    "plt.plot(x, (0.43 + x + 0.06*(x**2)) * b, 'k-', label='Fitted Crowding Distance')\n",
    "\n",
    "# Plot individual data\n",
    "plt.plot(mean_x_ecc, mean_cd_list, 'ko', alpha=0.1, label='Individual Crowding Distance')\n",
    "\n",
    "# Plot error bars\n",
    "plt.errorbar(eccentricities, mean_values, yerr=std_values, fmt='o', color='red', label='Mean ± Std')\n",
    "plt.fill_between(x, confidence_interval_cd[0], confidence_interval_cd[1], color='gray', alpha=0.3, label='95% Confidence Interval')\n",
    "\n",
    "plt.xlabel('Eccentricity (deg)')\n",
    "plt.ylabel('Crowding distance (deg)')\n",
    "#plt.yscale('log')\n",
    "plt.ylim(bottom=0.1)  # Set lower limit to 0.1 (10^-1)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0c570",
   "metadata": {},
   "source": [
    "## Fit Cortical Magnification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen = np.linspace(1, 11, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b663011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmag_basics(sid, h, label,\n",
    "                minecc=0, maxecc=12,\n",
    "                mincod=0.04):\n",
    "    \"\"\"Loads the basic cortical magnification data for a subject and returns it.\n",
    "    \n",
    "    The data is returned as ``(ecc, srf)`` where ``ecc`` is the eccentricity values\n",
    "    and ``srf`` is the surface area values within the given visual area.\n",
    "    \"\"\"\n",
    "    sub = cc.cmag.load_subject(sid)\n",
    "    hem = sub.hemis[h]\n",
    "    mask_nocod = {\n",
    "        'and': [\n",
    "            ('eccentricity', minecc, maxecc),\n",
    "            ('visual_area', label)]}\n",
    "    mask_cod = {'and': mask_nocod['and'] + [('variance_explained', mincod, 1)]}\n",
    "    rdat = ny.retinotopy_data(hem)\n",
    "    mask_ii = hem.mask(mask_cod)\n",
    "    ecc = rdat['eccentricity'][mask_ii]\n",
    "    srf = hem.prop('midgray_surface_area')\n",
    "    totarea = np.sum(srf[hem.mask(mask_nocod)])\n",
    "    srf = srf[mask_ii]\n",
    "    ii = np.argsort(ecc)\n",
    "    ecc = ecc[ii]\n",
    "    srf = srf[ii] * totarea / np.sum(srf)\n",
    "    return (ecc, srf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b45f72",
   "metadata": {},
   "source": [
    "$$ c_1 = \\sqrt{\\frac{a_M}{\\pi \\left(\\log\\left(\\frac{c_2 + M}{c_2}\\right) - \\frac{M}{c_2 + M}\\right)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8127d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cumarea(sid, h, label,\n",
    "                params0=(17.3, 0.75), fix_gain=False, method=None):\n",
    "    \"\"\"Given a subject, hemisphere, and label, fit the Horton and Hoyt (1991)\n",
    "    cortical magnification function to the retinotopic mapping data using the\n",
    "    method of cumulative area.\n",
    "    \"\"\"\n",
    "    (ecc,srf) = cmag_basics(sid, h, label)\n",
    "    if len(ecc) == 0:\n",
    "        raise RuntimeError(f\"no data found for {sid}:{h}:{label}\")\n",
    "    r = cc.cmag.HH91_fit_cumarea(\n",
    "        ecc, srf,\n",
    "        params0=params0,\n",
    "        fix_gain=fix_gain,\n",
    "        method=method)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a52e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dict(sid=[], h=[], label=[], a=[], b=[], loss=[])\n",
    "for sid in sids_orig:\n",
    "    print(sid)\n",
    "    for h in ['lh','rh']:\n",
    "        for lbl in [1,2,3,4]:\n",
    "            try:\n",
    "                r = fit_cumarea(sid, h, lbl)\n",
    "            except Exception as e:\n",
    "                print(f\"  - Skipping: {type(e)}\")\n",
    "                continue\n",
    "            df['sid'].append(sid)\n",
    "            df['h'].append(h)\n",
    "            df['label'].append(lbl)\n",
    "            df['a'].append(r.x[0])\n",
    "            df['b'].append(r.x[1])\n",
    "            df['loss'].append(r.fun)\n",
    "HH91_params = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2b054",
   "metadata": {},
   "source": [
    "## check the quality of the fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'lh': [[],[],[],[]], 'rh': [[],[],[],[]]}\n",
    "std_ecc = np.linspace(0.25, 12, 100)\n",
    "for sid in sids_orig:\n",
    "\n",
    "    hh91_fits = HH91_params[HH91_params['sid'] == sid]\n",
    "\n",
    "    for i, lbl in enumerate([1, 2, 3, 4]):\n",
    "        for j, hemi in enumerate(['lh','rh']):\n",
    "            #ax = axs[i, j]\n",
    "            hfit_row = hh91_fits[(hh91_fits['label'] == lbl) & (hh91_fits['h'] == hemi)]\n",
    "            if len(hfit_row) == 0:\n",
    "                continue\n",
    "\n",
    "            a = hfit_row['a'].values[0]\n",
    "            b = hfit_row['b'].values[0]\n",
    "\n",
    "            (ecc,srf) = cmag_basics(sid, hemi, lbl)\n",
    "            ii = np.argsort(ecc)\n",
    "            ecc = ecc[ii]\n",
    "            cum_area = np.cumsum(srf[ii])\n",
    "\n",
    "            cumarea_fit = cc.cmag.HH91_integral(ecc, a, b)\n",
    "            \n",
    "            cum_area = np.interp(std_ecc, ecc, cum_area)\n",
    "            cumarea_fit = np.interp(std_ecc, ecc, cumarea_fit)\n",
    "            \n",
    "            diff = cumarea_fit - cum_area\n",
    "            diff[std_ecc > np.max(ecc)] = np.nan\n",
    "            res[hemi][lbl - 1].append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = {\n",
    "    h: np.array(dat)\n",
    "    for (h,dat) in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(diffs['rh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, axs) = plt.subplots(4, 2, figsize=(7, 7), dpi=288, sharex=True, sharey=True)\n",
    "\n",
    "for i, lbl in enumerate([1, 2, 3, 4]):\n",
    "     for j, hemi in enumerate(['lh','rh']):\n",
    "        ax = axs[i, j]\n",
    "        diff_mtx = diffs[h][lbl - 1]\n",
    "        \n",
    "        ax.plot(std_ecc, np.nanmedian(diff_mtx, axis=0) / 100.0, color='k') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261da959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean a,b fits\n",
    "avg_ab = HH91_params.groupby(['label', 'h'])[['a', 'b']].mean()\n",
    "avg_ab = avg_ab.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(HH91_params.groupby(['label'])[['a', 'b']].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for showing that our fits in general for subjects are good:\n",
    "# (1) Use the average a and b parameters for the hemisphere/area to make the blue (normalized fit) lines\n",
    "# (2) Add an additional shaded region showing not the median error but the 75 percentile of the error in a lighter gray\n",
    "# (3) We should generate figures (like the next cell) for all subjects for the supplemental materials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ff123",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, axs) = plt.subplots(4, 2, figsize=(7, 7), dpi=288, sharex=True, sharey=True)\n",
    "\n",
    "for i, lbl in enumerate([1, 2, 3, 4]):\n",
    "    for j, hemi in enumerate(['lh', 'rh']):\n",
    "        ax = axs[i, j]\n",
    "        hfit_row = avg_ab[(avg_ab['label'] == lbl) & (avg_ab['h'] == hemi)]\n",
    "        a = hfit_row['a'].values[0]\n",
    "        b = hfit_row['b'].values[0]\n",
    "\n",
    "        (ecc, srf) = cmag_basics(sid, hemi, lbl)\n",
    "        ii = np.argsort(ecc)\n",
    "        ecc = ecc[ii]\n",
    "        cum_area = np.cumsum(srf[ii])\n",
    "\n",
    "        cumarea_fit = cc.cmag.HH91_integral(ecc, a, b)\n",
    "        cumarea_fit = np.interp(std_ecc, ecc, cumarea_fit)\n",
    "\n",
    "        diff_mtx = diffs[hemi][lbl - 1]\n",
    "        ii = np.isfinite(diff_mtx)\n",
    "        diff_mtx[ii] = np.abs(diff_mtx[ii])\n",
    "        diff = np.nanmedian(diff_mtx, axis=0)\n",
    "        diff_95 = np.nanpercentile(diff_mtx, 95, axis=0)\n",
    "\n",
    "        ax.plot(std_ecc, cumarea_fit/100, label=\"H&H Model Fit\", color='blue') \n",
    "        ax.fill_between(std_ecc, (cumarea_fit - diff)/100, (cumarea_fit + diff)/100, color='0.5', alpha=0.7, label='Median Error')\n",
    "        ax.fill_between(std_ecc, (cumarea_fit - diff_95)/100, (cumarea_fit + diff_95)/100, color='0.3', alpha=0.3, label='95% Error')\n",
    "\n",
    "        axs[3,0].set_xlabel('Eccentricity [degree]')\n",
    "        axs[3,1].set_xlabel('Eccentricity [degree]')\n",
    "        ax.set_ylabel(r'Surface Area [cm$^2$]')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        axs[0, 1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b44b8",
   "metadata": {},
   "source": [
    "## new code on 9.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_bounds_from_abs_ranking(diff_mtx, pct):\n",
    "    # shape is (40,100) here\n",
    "    n_subj, n_ecc = diff_mtx.shape\n",
    "    low  = np.full(n_ecc, np.nan)\n",
    "    high = np.full(n_ecc, np.nan)\n",
    "\n",
    "    for k in range(n_ecc):\n",
    "        # diffs across subjects at that single eccentricity\n",
    "        col = diff_mtx[:, k]\n",
    "        col = col[np.isfinite(col)]\n",
    "\n",
    "        # assign a percentile rank (based on abs(diff))\n",
    "        ordering = np.argsort(np.abs(col))\n",
    "        ranks = np.linspace(0, 100, len(col))\n",
    "        # avoid overwrite in place\n",
    "        percentiles = np.empty_like(ranks)\n",
    "        percentiles[ordering] = ranks\n",
    "\n",
    "        # keep those within the target percentile\n",
    "        subset = col[percentiles <= pct]\n",
    "        if subset.size == 0:\n",
    "            continue\n",
    "        low[k]  = np.min(subset)\n",
    "        high[k] = np.max(subset)\n",
    "\n",
    "    return low, high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6ea55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(fig, axs) = plt.subplots(4, 2, figsize=(7, 7), dpi=288, sharex=True, sharey=True)\n",
    "\n",
    "for i, lbl in enumerate([1, 2, 3, 4]):\n",
    "    for j, hemi in enumerate(['lh', 'rh']):\n",
    "        ax = axs[i, j]\n",
    "\n",
    "        hfit_row = avg_ab[(avg_ab['label'] == lbl) & (avg_ab['h'] == hemi)]\n",
    "        a = hfit_row['a'].values[0]\n",
    "        b = hfit_row['b'].values[0]\n",
    "\n",
    "        (ecc, srf) = cmag_basics(sid, hemi, lbl)\n",
    "        ii = np.argsort(ecc)\n",
    "        ecc = ecc[ii]\n",
    "        cum_area = np.cumsum(srf[ii])\n",
    "\n",
    "        cumarea_fit = cc.cmag.HH91_integral(ecc, a, b)\n",
    "        cumarea_fit = np.interp(std_ecc, ecc, cumarea_fit)\n",
    "\n",
    "        diff_mtx = diffs[hemi][lbl - 1]\n",
    "        diff_mtx = np.where(np.isfinite(diff_mtx), diff_mtx, np.nan)\n",
    "\n",
    "        # abs-percentile bounds\n",
    "        med_low, med_high = signed_bounds_from_abs_ranking(diff_mtx, 50)\n",
    "        p95_low, p95_high = signed_bounds_from_abs_ranking(diff_mtx, 95)\n",
    "\n",
    "        # scale to cumulative-area units\n",
    "        y_med_low  = (cumarea_fit + med_low) / 100\n",
    "        y_med_high = (cumarea_fit + med_high) / 100\n",
    "        y_p95_low  = (cumarea_fit + p95_low) / 100\n",
    "        y_p95_high = (cumarea_fit + p95_high) / 100\n",
    "\n",
    "        ax.plot(std_ecc, cumarea_fit/100, color='blue', label=\"H&H Model Fit\")\n",
    "\n",
    "        # dark gray: 50% \n",
    "        ax.fill_between(std_ecc, y_med_low, y_med_high, color='0.5', alpha=0.7, label='Median Error')\n",
    "\n",
    "        # light gray: 95% \n",
    "        ax.fill_between(std_ecc, y_p95_low, y_p95_high, color='0.3', alpha=0.3, label='95% Error')\n",
    "        \n",
    "        axs[3,0].set_xlabel('Eccentricity [degree]')\n",
    "        axs[3,1].set_xlabel('Eccentricity [degree]')\n",
    "        ax.set_ylabel(r'Surface Area [cm$^2$]')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        if i == 0 and j == 1:\n",
    "            ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f098631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2aa652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell: make ready for paper and make a figure for each subject\n",
    "# (can save in the figures directory: /data/crowding/figures)\n",
    "\n",
    "save_dir = os.path.expanduser('~/for_crowding_figures')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for sid in sids_orig:\n",
    "    hh91_fits = HH91_params[HH91_params['sid'] == sid]\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(7, 7), dpi=72*8, sharex=True, sharey=True)\n",
    "\n",
    "    for i, lbl in enumerate([1, 2, 3, 4]):\n",
    "        for j, hemi in enumerate(['lh', 'rh']):\n",
    "            ax = axs[i, j]\n",
    "            hfit_row = hh91_fits[(hh91_fits['label'] == lbl) & (hh91_fits['h'] == hemi)]\n",
    "            if len(hfit_row) == 0:\n",
    "                continue\n",
    "\n",
    "            a = hfit_row['a'].values[0]\n",
    "            b = hfit_row['b'].values[0]\n",
    "            ecc, srf = cmag_basics(sid, hemi, lbl)\n",
    "            ii = np.argsort(ecc)\n",
    "            ecc = ecc[ii]\n",
    "            cum_area = np.cumsum(srf[ii])\n",
    "            cumarea_fit = cc.cmag.HH91_integral(ecc, a, b)\n",
    "\n",
    "            ax.plot(ecc, cumarea_fit / 100, label=\"H&H Model Fit\", color='blue') \n",
    "            ax.plot(ecc, cum_area / 100, label=\"Cumulative Surface Area\", color='gray')\n",
    "            ax.set_ylabel(r'Surface Area [cm$^2$]')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "\n",
    "    axs[0, 1].legend()\n",
    "    axs[3, 0].set_xlabel('Eccentricity [degree]')\n",
    "    axs[3, 1].set_xlabel('Eccentricity [degree]')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    fig_path = os.path.join(save_dir, f'fits_qc_{sid}.png')\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdb4b6",
   "metadata": {},
   "source": [
    "### Histogram of the `a` parameter (gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = HH91_params\n",
    "# (fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "# for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "#     df1 = df[df['label'] == lbl]\n",
    "#     for (h,ax) in zip(['lh','rh'], axrow):\n",
    "#         df2 = df1[df1['h'] == h]\n",
    "#         q = df2['a'].values\n",
    "#         ax.hist(q)\n",
    "#         ax.set_xlim([0,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ca7d",
   "metadata": {},
   "source": [
    "### Histogram of the `b` parameter (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = HH91_params\n",
    "# (fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "# for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "#     df1 = df[df['label'] == lbl]\n",
    "#     for (h,ax) in zip(['lh','rh'], axrow):\n",
    "#         df2 = df1[df1['h'] == h]\n",
    "#         q = df2['b'].values\n",
    "#         ax.hist(q)\n",
    "#         #ax.set_xlim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ee13e",
   "metadata": {},
   "source": [
    "### plot hh91 fit vs eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = HH91_params\n",
    "# (fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "# fig.subplots_adjust(0,0,1,1,0.1,0.1)\n",
    "# eccrng = np.linspace(0.5, 11, 500)\n",
    "# for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "#     df1 = df[df['label'] == lbl]\n",
    "#     for (h,ax) in zip(['lh','rh'], axrow):\n",
    "#         df2 = df1[df1['h'] == h]\n",
    "#         for (a,b) in zip(df2['a'].values, df2['b'].values):\n",
    "#             ax.loglog(eccrng, (a / (b + eccrng))**2, 'k-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = HH91_params\n",
    "# (fig,axs) = plt.subplots(1,2, figsize=(4,1.5), dpi=288, sharex=True, sharey=True)\n",
    "# eccrng = np.linspace(0.5, 12, 500)\n",
    "# for (lbl,clr) in zip([1,2,3,4], ['r','g','b','k']):\n",
    "#     df1 = df[df['label'] == lbl]\n",
    "#     for (h,ax) in zip(['lh','rh'], axs):\n",
    "#         df2 = df1[df1['h'] == h]\n",
    "#         a = df2['a'].values\n",
    "#         b = df2['b'].values\n",
    "#         m = (a[:,None] / (b[:,None] + eccrng[None,:]))**2\n",
    "#         mu = np.mean(m, axis=0)\n",
    "#         sd = np.std(m, axis=0)\n",
    "#         ax.fill_between(eccrng, mu - sd, mu + sd, edgecolor=None, facecolor=clr, alpha=0.2, zorder=-1)\n",
    "#         ax.loglog(eccrng, mu, clr+'-', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85753c7a",
   "metadata": {},
   "source": [
    "### bootstrap on C.Mag fits and replot the C.Mag plot (similar to the one above) using confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 11, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_params = HH91_params.groupby('label')[['a', 'b']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmag_per_label = {}\n",
    "\n",
    "# fitted Cmag for each area\n",
    "for _, row in mean_params.iterrows():\n",
    "    label = row['label']\n",
    "    a = row['a']\n",
    "    b = row['b']\n",
    "    cmag_r = (a / (b + x))**2\n",
    "    cmag_per_label[label] = cmag_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41450967",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(HH91_params.groupby('label')[['a', 'b']].agg(['mean', 'std']).reset_index(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstrap_samples = 1000\n",
    "\n",
    "bootstrap_cmag_per_label = {}\n",
    "\n",
    "for lbl in [1,2,3,4]:\n",
    "    df_lbl = HH91_params[HH91_params['label']==lbl]\n",
    "    a_values = df_lbl['a'].values\n",
    "    b_values = df_lbl['b'].values\n",
    "\n",
    "    # Subjects added to keep track\n",
    "    sids = df_lbl['sid'].values\n",
    "\n",
    "    boot_curves = []\n",
    "    n = len(a_values)\n",
    "\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "        mean_a = np.mean(a_values[indices])\n",
    "        mean_b = np.mean(b_values[indices])\n",
    "\n",
    "        # cmag curve\n",
    "        cmag_boot = (mean_a / (mean_b + x))**2\n",
    "        boot_curves.append(cmag_boot)\n",
    "\n",
    "    # shape (1000, 1000)\n",
    "    boot_curves = np.array(boot_curves)\n",
    "    bootstrap_cmag_per_label[lbl] = boot_curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_v1 = bootstrap_cmag_per_label[1]\n",
    "bootstrapped_v2 = bootstrap_cmag_per_label[2]\n",
    "bootstrapped_v3 = bootstrap_cmag_per_label[3]\n",
    "bootstrapped_v4 = bootstrap_cmag_per_label[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval_v1 = np.percentile(bootstrapped_v1, [2.5, 97.5], axis=0)\n",
    "confidence_interval_v2 = np.percentile(bootstrapped_v2, [2.5, 97.5], axis=0)\n",
    "confidence_interval_v3 = np.percentile(bootstrapped_v3, [2.5, 97.5], axis=0)\n",
    "confidence_interval_v4 = np.percentile(bootstrapped_v4, [2.5, 97.5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(3.5, 3.5), dpi=72*8)\n",
    "\n",
    "# Plotting the fitted lines for each visual area\n",
    "ax.plot(x, np.sqrt(cmag_per_label[1]/2), 'k', label='V1 fitted line')\n",
    "ax.plot(x, np.sqrt(cmag_per_label[2]/2), 'r', label='V2 fitted line')\n",
    "ax.plot(x, np.sqrt(cmag_per_label[3]/2), 'm', label='V3 fitted line')\n",
    "ax.plot(x, np.sqrt(cmag_per_label[4]/2), 'cyan', label='hV4 fitted line')\n",
    "\n",
    "# Plotting the confidence intervals for each visual area\n",
    "ax.fill_between(x, \n",
    "                 np.sqrt(confidence_interval_v1[0]/2),\n",
    "                 np.sqrt(confidence_interval_v1[1]/2),\n",
    "                 color='k', alpha=0.3)\n",
    "ax.fill_between(x, \n",
    "                 np.sqrt(confidence_interval_v2[0]/2),\n",
    "                 np.sqrt(confidence_interval_v2[1]/2),\n",
    "                 color='r', alpha=0.3)\n",
    "ax.fill_between(x, \n",
    "                 np.sqrt(confidence_interval_v3[0]/2),\n",
    "                 np.sqrt(confidence_interval_v3[1]/2),\n",
    "                 color='m', alpha=0.3)\n",
    "ax.fill_between(x, \n",
    "                 np.sqrt(confidence_interval_v4[0]/2),\n",
    "                 np.sqrt(confidence_interval_v4[1]/2),\n",
    "                 color='cyan', alpha=0.3)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Eccentricity [deg]\")\n",
    "ax.set_ylabel(\"Radial Cortical Magnification [mm/deg]\")\n",
    "#ax.set_title(\"Average Radial Cortical Magnification for V1, V2, V3, hV4\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a8057",
   "metadata": {},
   "source": [
    "### cortical crowding distance (ccd) and coefficent of variation for ccd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_ccd_1 = []\n",
    "bootstrapped_ccd_2 = []\n",
    "bootstrapped_ccd_3 = []\n",
    "bootstrapped_ccd_4 = []\n",
    "\n",
    "# Iterate through 1000 bootstrapped samples\n",
    "for i in range(1000):\n",
    "    ccd_v1 = bootstrapped[i] * np.sqrt(bootstrapped_v1[i] / 2)\n",
    "    bootstrapped_ccd_1.append(ccd_v1)\n",
    "    \n",
    "    ccd_v2 = bootstrapped[i] * np.sqrt(bootstrapped_v2[i] / 2)\n",
    "    bootstrapped_ccd_2.append(ccd_v2)\n",
    "    \n",
    "    ccd_v3 = bootstrapped[i] * np.sqrt(bootstrapped_v3[i] / 2)\n",
    "    bootstrapped_ccd_3.append(ccd_v3)\n",
    "    \n",
    "    ccd_v4 = bootstrapped[i] * np.sqrt(bootstrapped_v4[i] / 2)\n",
    "    bootstrapped_ccd_4.append(ccd_v4)\n",
    "\n",
    "bootstrapped_ccd_1 = np.array(bootstrapped_ccd_1)\n",
    "bootstrapped_ccd_2 = np.array(bootstrapped_ccd_2)\n",
    "bootstrapped_ccd_3 = np.array(bootstrapped_ccd_3)\n",
    "bootstrapped_ccd_4 = np.array(bootstrapped_ccd_4)\n",
    "\n",
    "# Calculate the mean of bootstrapped CCD values for each visual area\n",
    "ccd1 = np.mean(bootstrapped_ccd_1, axis=0)\n",
    "ccd2 = np.mean(bootstrapped_ccd_2, axis=0)\n",
    "ccd3 = np.mean(bootstrapped_ccd_3, axis=0)\n",
    "ccd4 = np.mean(bootstrapped_ccd_4, axis=0)\n",
    "\n",
    "# Calculate the confidence interval for bootstrapped CCD values for each visual area\n",
    "confidence_interval_ccd_1 = np.percentile(bootstrapped_ccd_1,  [16, 84], axis=0)\n",
    "confidence_interval_ccd_2 = np.percentile(bootstrapped_ccd_2,  [16, 84], axis=0)\n",
    "confidence_interval_ccd_3 = np.percentile(bootstrapped_ccd_3,  [16, 84], axis=0)\n",
    "confidence_interval_ccd_4 = np.percentile(bootstrapped_ccd_4,  [16, 84], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2773ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_ccd_1, mean_ccd_2,mean_ccd_3,mean_ccd_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coefficient of variation for ccd_1\n",
    "mean_ccd_1 = np.mean(ccd1)\n",
    "std_ccd_1 = np.std(ccd1)\n",
    "cv_ccd_1 = std_ccd_1 / mean_ccd_1\n",
    "rounded_cv_ccd_1 = round(cv_ccd_1, 3)\n",
    "\n",
    "print(\"Coefficient of Variation (CCD 1):\", rounded_cv_ccd_1)\n",
    "\n",
    "# Calculate the coefficient of variation for ccd_2\n",
    "mean_ccd_2 = np.mean(ccd2)\n",
    "std_ccd_2 = np.std(ccd2)\n",
    "cv_ccd_2 = std_ccd_2 / mean_ccd_2\n",
    "rounded_cv_ccd_2 = round(cv_ccd_2, 3)\n",
    "print(\"Coefficient of Variation (CCD 2):\", rounded_cv_ccd_2)\n",
    "\n",
    "# Calculate the coefficient of variation for ccd_3\n",
    "mean_ccd_3 = np.mean(ccd3)\n",
    "std_ccd_3 = np.std(ccd3)\n",
    "cv_ccd_3 = std_ccd_3 / mean_ccd_3\n",
    "rounded_cv_ccd_3 = round(cv_ccd_3, 3)\n",
    "print(\"Coefficient of Variation (CCD 3):\", rounded_cv_ccd_3)\n",
    "\n",
    "# Calculate the coefficient of variation for ccd_4\n",
    "mean_ccd_4 = np.mean(ccd4)\n",
    "std_ccd_4 = np.std(ccd4)\n",
    "cv_ccd_4 = std_ccd_4 / mean_ccd_4\n",
    "rounded_cv_ccd_4 = round(cv_ccd_4, 3)\n",
    "print(\"Coefficient of Variation (CCD 4):\", rounded_cv_ccd_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_cv(data, num_samples=1000):\n",
    "    bootstrapped_cv = []\n",
    "    n = len(data)\n",
    "    for _ in range(num_samples):\n",
    "        sample_indices = np.random.choice(n, size=n, replace=True)\n",
    "        bootstrapped_sample = data[sample_indices]\n",
    "        mean_sample = np.mean(bootstrapped_sample)\n",
    "        std_sample = np.std(bootstrapped_sample)\n",
    "        cv_sample = std_sample / mean_sample\n",
    "        bootstrapped_cv.append(cv_sample)\n",
    "    return np.array(bootstrapped_cv)\n",
    "\n",
    "bootstrapped_cv_1 = bootstrap_cv(ccd1)\n",
    "ci_1 = np.percentile(bootstrapped_cv_1, [2.5, 97.5])\n",
    "\n",
    "bootstrapped_cv_2 = bootstrap_cv(ccd2)\n",
    "ci_2 = np.percentile(bootstrapped_cv_2, [2.5, 97.5])\n",
    "\n",
    "bootstrapped_cv_3 = bootstrap_cv(ccd3)\n",
    "ci_3 = np.percentile(bootstrapped_cv_3, [2.5, 97.5])\n",
    "\n",
    "bootstrapped_cv_4 = bootstrap_cv(ccd4)\n",
    "ci_4 = np.percentile(bootstrapped_cv_4, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65015f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ccd_1 = bootstrapped_cv_1.mean()\n",
    "cv_ccd_2 = bootstrapped_cv_2.mean()\n",
    "cv_ccd_3 = bootstrapped_cv_3.mean()\n",
    "cv_ccd_4 = bootstrapped_cv_4.mean()\n",
    "print([cv_ccd_1, cv_ccd_2, cv_ccd_3, cv_ccd_4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(ci_1, 2))\n",
    "print(np.round(ci_2, 2))\n",
    "print(np.round(ci_3, 2))\n",
    "print(np.round(ci_4, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d278dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ccd_values = [cv_ccd_1, cv_ccd_2, cv_ccd_3, cv_ccd_4]\n",
    "ccd_labels = ['V1', 'V2', 'V3', 'V4']\n",
    "cv_ci_list = [(ci_1[0], ci_1[1]), (ci_2[0], ci_2[1]), (ci_3[0], ci_3[1]),(ci_4[0], ci_4[1])]\n",
    "\n",
    "lower_bound = [ci[0] for ci in cv_ci_list]\n",
    "upper_bound = [ci[1] for ci in cv_ci_list]\n",
    "\n",
    "yerr = [[cv_ccd_values[i] - lower_bound[i] for i in range(len(cv_ccd_values))],\n",
    "        [upper_bound[i] - cv_ccd_values[i] for i in range(len(cv_ccd_values))]]\n",
    "\n",
    "plt.figure(figsize=(3.5, 3))\n",
    "plt.bar(ccd_labels, cv_ccd_values, yerr=yerr, capsize=5, color=['grey', 'red', 'magenta', 'cyan'])\n",
    "\n",
    "plt.xlabel('Visual Areas')\n",
    "plt.ylabel('Coefficient of Variation')\n",
    "plt.title('Coefficient of Variation for Cortical Crowding Distance')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e21bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean cortical crowding distance of each area\n",
    "plt.figure(figsize=(3.5, 3))\n",
    "plt.plot(x, ccd1, label='Cortical crowding distance in V1', color='black')\n",
    "plt.plot(x, ccd2, label='Cortical crowding distance in V2', color='red')\n",
    "plt.plot(x, ccd3, label='Cortical crowding distance in V3', color='magenta')\n",
    "plt.plot(x, ccd4, label='Cortical crowding distance in hV4', color='cyan')\n",
    "\n",
    "plt.fill_between(x, confidence_interval_ccd_1[0], confidence_interval_ccd_1[1], color='black', alpha=0.3)\n",
    "plt.fill_between(x, confidence_interval_ccd_2[0], confidence_interval_ccd_2[1], color='red', alpha=0.3)\n",
    "plt.fill_between(x, confidence_interval_ccd_3[0], confidence_interval_ccd_3[1], color='magenta', alpha=0.3)\n",
    "plt.fill_between(x, confidence_interval_ccd_4[0], confidence_interval_ccd_4[1], color='cyan', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.xlabel('Eccentricity (deg)')\n",
    "plt.ylabel('Cortical Crowding Distance (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the cortical crowding distance\n",
    "plt.figure(figsize=(3.5, 3))\n",
    "plt.plot(x, ccd1/ccd1[0], color='black')\n",
    "plt.plot(x, ccd2/ccd2[0], color='red')\n",
    "plt.plot(x, ccd3/ccd3[0], color='magenta')\n",
    "plt.plot(x, ccd4/ccd4[0], color='cyan')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.xlabel('Eccentricity (deg)')\n",
    "plt.ylabel('Normalized Cortical C.D. (mm)')\n",
    "#plt.xticks([2.5, 5, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927de95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c59fbd",
   "metadata": {},
   "source": [
    "## have two dfs of fitted cortical magnification and crowding values based on VM/HM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8349c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 sessions, so now each sub have 12 cd (match with polar angle) instead of 24\n",
    "# mean_cd_polar = df.groupby(['ID','Eccen_X','Eccen_Y'])['CrowdingDistance'].apply(gmean).reset_index()\n",
    "mean_cd_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at horizontal meridian for crowding_hrz\n",
    "# h is assigned 'lh' for x values larger than 0 and 'rh' otherwise\n",
    "crowding_hrz = mean_cd_polar[mean_cd_polar['Eccen_Y'] == 0].copy()\n",
    "x = crowding_hrz['Eccen_X'].values\n",
    "# for calculating cmag, have a col for abs(x)\n",
    "crowding_hrz['eccen'] = np.abs(x)\n",
    "crowding_hrz['h'] = np.where(x > 0, 'lh', 'rh')\n",
    "crowding_hrz.rename(columns={'CrowdingDistance': 'CrowdingDistance_HM'}, inplace=True)\n",
    "crowding_hrz.rename(columns={'ID': 'sid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at vertical meridian for crowding_vrt\n",
    "crowding_vrt = mean_cd_polar[mean_cd_polar['Eccen_X'] == 0].copy()\n",
    "y = crowding_vrt['Eccen_Y'].values\n",
    "crowding_vrt['eccen'] = np.abs(y)\n",
    "\n",
    "# Create the 'CrowdingDistance_UVM' and 'CrowdingDistance_LVM' columns\n",
    "crowding_vrt['CrowdingDistance_UVM'] = np.where(y > 0, np.nan, crowding_vrt['CrowdingDistance'])\n",
    "crowding_vrt['CrowdingDistance_LVM'] = np.where(y > 0, crowding_vrt['CrowdingDistance'], np.nan)\n",
    "crowding_vrt.rename(columns={'ID': 'sid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be74eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create duplicate rows with 'h' column\n",
    "crowding_vrt_lh = crowding_vrt.copy()\n",
    "crowding_vrt_lh['h'] = 'lh'\n",
    "crowding_vrt_rh = crowding_vrt.copy()\n",
    "crowding_vrt_rh['h'] = 'rh'\n",
    "\n",
    "# Combine the two DataFrames, so now crowding_vrt has two rows with same data except one row lh and one row rh\n",
    "crowding_vrt = pd.concat([crowding_vrt_lh, crowding_vrt_rh], ignore_index=True)\n",
    "crowding_vrt = crowding_vrt.drop(columns=['CrowdingDistance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee551629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params.copy()\n",
    "\n",
    "# will have three rows for diff ecc, but the same a,b params\n",
    "df = df.merge(\n",
    "    pd.DataFrame(dict(eccen=[2.5, 5.0, 10.0])),\n",
    "    how='cross')\n",
    "\n",
    "a = df['a']\n",
    "b = df['b']\n",
    "ecc = df['eccen']\n",
    "# calculate cmag based on a,b params from HH91_params\n",
    "df['cmag_fit'] = (a / (ecc + b))**2\n",
    "# add 1d visual magnification\n",
    "df['vmag1d_fit'] = np.sqrt(1 / df['cmag_fit'])\n",
    "\n",
    "# add CrowdingDistance_HM to df by merging it with crowding_hrz\n",
    "# _full versions have x, y positions too\n",
    "df_HM_full = df.merge(crowding_hrz, on=['sid', 'eccen','h'])\n",
    "# 15 subs, 2 hemisphere * 3 ecc * 4 visual area = 360 rows\n",
    "df_HM = df_HM_full.drop(columns=['Eccen_X', 'Eccen_Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM_full = df.merge(crowding_vrt, on=['sid', 'eccen','h'])\n",
    "# 15 subs, 2 loc * 3 ecc * 4 visual area * 2h = 48\n",
    "df_VM = df_VM_full.drop(columns=['Eccen_X', 'Eccen_Y'])\n",
    "\n",
    "df_VM = df_VM.groupby(\n",
    "    ['sid', 'label', 'eccen'], as_index=False\n",
    ").agg({\n",
    "    'a': 'mean',\n",
    "    'b': 'mean',\n",
    "    'loss': 'mean',\n",
    "    'CrowdingDistance_UVM': 'mean', \n",
    "    'CrowdingDistance_LVM': 'mean', \n",
    "})\n",
    "\n",
    "# compute cmag_fit based on a, b, and eccen\n",
    "df_VM['cmag_fit'] = (df_VM['a'] / (df_VM['eccen'] + df_VM['b']))**2\n",
    "\n",
    "# compute 1d_vmag_fit from cmag_fit\n",
    "df_VM['vmag1d_fit'] = np.sqrt(1 / df_VM['cmag_fit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd354205",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_VM = dict(\n",
    "    a='a_VM', b='b_VM', loss='loss_VM',\n",
    "    cmag_fit='cmag_fit_VM', vmag1d_fit='vmag1d_fit_VM')\n",
    "rename_HM = dict(\n",
    "    a='a_HM', b='b_HM', loss='loss_HM',\n",
    "    cmag_fit='cmag_fit_HM', vmag1d_fit='vmag1d_fit_HM')\n",
    "\n",
    "df = pd.merge(\n",
    "    df_HM.rename(columns=rename_HM), df_VM.rename(columns=rename_VM),\n",
    "    on=('sid', 'label', 'eccen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM[\"CD_VM\"] = (df_VM[\"CrowdingDistance_UVM\"] + df_VM[\"CrowdingDistance_LVM\"]) / 2\n",
    "\n",
    "df_VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH91_params_bi = HH91_params.groupby(\n",
    "    ['sid', 'label'], as_index=False\n",
    "    ).agg({\n",
    "    'a': 'mean',\n",
    "    'b': 'mean',\n",
    "    'loss': 'mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH91_params_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d278fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = HH91_params_bi.merge(\n",
    "    pd.DataFrame(dict(eccen=[2.5, 5.0, 10.0])),\n",
    "    how='cross')\n",
    "\n",
    "a = df_mean['a']\n",
    "b = df_mean['b']\n",
    "ecc = df_mean['eccen']\n",
    "# calculate cmag based on a,b params from HH91_params\n",
    "df_mean['cmag_fit'] = (a / (ecc + b))**2\n",
    "# add 1d visual magnification\n",
    "df_mean['vmag1d_fit'] = np.sqrt(1 / df_mean['cmag_fit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cd_polar[\"eccen\"] = mean_cd_polar.apply(\n",
    "    lambda row: abs(row[\"Eccen_X\"]) if row[\"Eccen_X\"] != 0 else abs(row[\"Eccen_Y\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ad40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cd_polar = mean_cd_polar.rename(columns={\"ID\": \"sid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa514ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = mean_cd_polar.drop(columns=[\"Eccen_X\", \"Eccen_Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea88ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp.groupby(['sid','eccen'])['CrowdingDistance'].apply(gmean).reset_index() #each subject three cd value, at three ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df_mean.merge(tmp, on=['sid', 'eccen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean #15 subj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3118c2",
   "metadata": {},
   "source": [
    "## not being used now: for each subject, each visual area, to have a fitted cortical crowding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_loss_notused(params, cd, cmags, position):\n",
    "    '''\n",
    "    position: 1 for horizontal, 2 for upper, 3 for lower\n",
    "    params: d for cortical crowding distance, g_h, g_u, g_l\n",
    "    cd: crowding distance\n",
    "    cmags: cortical magnification\n",
    "    '''\n",
    "    gain = params[position] ** 2  # ensure that gain >= 0\n",
    "    d = params[0]\n",
    "    pred = d * np.sqrt(2/(gain*cmags))\n",
    "    \n",
    "    return np.sum((cd - pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferred for now\n",
    "def crowding_loss(params, cd, cmags, position):\n",
    "    '''\n",
    "    position: 1 for horizontal, 2 for upper, 3 for lower\n",
    "    params: d, g_h, g_u, g_l\n",
    "    cd: crowding distance\n",
    "    cmags: cortical magnification\n",
    "    '''\n",
    "    gain = np.exp(params[position])\n",
    "    d = params[0]\n",
    "    pred = d * np.sqrt(2/(gain*cmags))\n",
    "    \n",
    "    return np.sum((cd - pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_fit_data(df_HM, df_VM, sid, label):\n",
    "    sub_hm = df_HM[(df_HM['label']==label) & (df_HM['sid']==sid)]\n",
    "    cd_hm = sub_hm['CrowdingDistance_HM'].values\n",
    "    cm_hm = sub_hm['cmag_fit'].values\n",
    "    pos_hm = np.ones(len(cd_hm),dtype=int)\n",
    "    \n",
    "    sub_vm = df_VM[(df_VM['label']==label) & (df_VM['sid']==sid)]\n",
    "    cd_um = sub_vm['CrowdingDistance_UVM'].values\n",
    "    cd_lm = sub_vm['CrowdingDistance_LVM'].values\n",
    "    cm_um = sub_vm['cmag_fit'].values\n",
    "    cm_lm = sub_vm['cmag_fit'].values\n",
    "    pos_um = np.ones(len(cd_um),dtype=int)*2\n",
    "    pos_lm = np.ones(len(cd_lm),dtype=int)*3\n",
    "    \n",
    "    return(np.concatenate([cd_hm,cd_um,cd_lm]), np.concatenate([cm_hm,cm_um,cm_lm]), np.concatenate([pos_hm,pos_um,pos_lm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowding_fit_data(df_HM, df_VM, 'sub-wlsubj158', 1) # 12 vals, 6+3+3, hm: lh*3, rh*3, vm: mean for 3 ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_fit_notused(df_HM, df_VM, sid, label, method = 'BFGS'):\n",
    "    from scipy.optimize import minimize\n",
    "    cd_cm = crowding_fit_data(df_HM, df_VM, sid, label)\n",
    "    params0 = [1.5, np.sqrt(1), np.sqrt(1), np.sqrt(1)] \n",
    "    r = minimize(crowding_loss, params0, args=cd_cm, method=method)\n",
    "    r.x[1] = r.x[1]**2\n",
    "    r.x[2] = r.x[2]**2\n",
    "    r.x[3] = r.x[3]**2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577be830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_fit(df_HM, df_VM, sid, label, method = 'BFGS'):\n",
    "    from scipy.optimize import minimize\n",
    "    cd_cm = crowding_fit_data(df_HM, df_VM, sid, label)\n",
    "    params0 = [1.5, np.log(1), np.log(1), np.log(1)] \n",
    "    r = minimize(crowding_loss, params0, args=cd_cm, method=method)\n",
    "    r.x[1] = np.exp(r.x[1])\n",
    "    r.x[2] = np.exp(r.x[2])\n",
    "    r.x[3] = np.exp(r.x[3])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit = dict(sid=[], label=[], d=[], g_h=[], g_u=[], g_l=[], loss=[])\n",
    "sids = np.unique(df_HM['sid'])\n",
    "for sid in sids:\n",
    "    for lbl in [1,2,3,4]:\n",
    "        r = crowding_fit(df_HM, df_VM, sid, lbl)\n",
    "        df_fit['sid'].append(sid)\n",
    "        df_fit['label'].append(lbl)\n",
    "        df_fit['d'].append(r.x[0])\n",
    "        df_fit['g_h'].append(r.x[1])\n",
    "        df_fit['g_u'].append(r.x[2])\n",
    "        df_fit['g_l'].append(r.x[3])\n",
    "        df_fit['loss'].append(r.fun)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b73441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fit_square = pd.DataFrame(df_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp = pd.DataFrame(df_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9caf298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp.groupby('label')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp.groupby('label')['d'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH91_params.groupby('label')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37976347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_cv(data, num_samples=1000):\n",
    "    bootstrapped_cv = []\n",
    "    n = len(data)\n",
    "    for _ in range(num_samples):\n",
    "        sample_indices = np.random.choice(n, size=n, replace=True)\n",
    "        bootstrapped_sample = data[sample_indices]\n",
    "        mean_sample = np.mean(bootstrapped_sample)\n",
    "        std_sample = np.std(bootstrapped_sample)\n",
    "        cv_sample = std_sample / mean_sample\n",
    "        bootstrapped_cv.append(cv_sample)\n",
    "    return np.array(bootstrapped_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff501ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp.groupby('label')['d'].std()/df_fit_exp.groupby('label')['d'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_values = []\n",
    "ci_list = []\n",
    "\n",
    "for label in [1,2,3,4]:\n",
    "    data = df_fit_exp[df_fit_exp['label'] == label]['d'].values\n",
    "    bootstrapped = bootstrap_cv(data)\n",
    "    mean_cv = bootstrapped.mean()\n",
    "    ci = np.percentile(bootstrapped, [2.5, 97.5])\n",
    "    \n",
    "    cv_values.append(mean_cv)\n",
    "    ci_list.append(ci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = [ci[0] for ci in ci_list]\n",
    "upper_bounds = [ci[1] for ci in ci_list]\n",
    "\n",
    "yerr = [\n",
    "    [mean_cv - lb for mean_cv, lb in zip(cv_values, lower_bounds)],\n",
    "    [ub - mean_cv for mean_cv, ub in zip(cv_values, upper_bounds)]\n",
    "]\n",
    "\n",
    "labels = ['1', '2', '3', '4'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5746ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(labels, cv_values, yerr=yerr, capsize=5, color=['grey', 'red', 'magenta', 'green'])\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Coefficient of Variation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b556d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b351530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp\n",
    "sns.violinplot(data=df_fit_exp,x='label',y='d',bw=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_fit_exp,x='label',y='loss',bw=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b936de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots based on HM measurements\n",
    "fig, axs = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(0,0,1,1,0.1,0.3)\n",
    "\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df_HM[df_HM['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        loss = df2['loss']\n",
    "        vmag = df2['1d_vmag_fit']\n",
    "        cd = df2['CrowdingDistance_HM']\n",
    "        eccen = df2['eccen']\n",
    "        for (e,color) in zip([2.5,5,10],'rgb'):\n",
    "            ii = (eccen==e)\n",
    "            ax.scatter(vmag[ii], cd[ii], s=4*(2-loss[ii]/2000), c=color)\n",
    "            ax.plot([0,2],[0,2],'k-')\n",
    "            ax.set_title(f'{h} Label={lbl}', fontsize=6)\n",
    "            \n",
    "fig.text(0.5, -0.1, '1D Vmag Fit', ha='center', fontsize=8)\n",
    "fig.text(-0.1, 0.5, 'Crowding Distance', va='center', rotation='vertical', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85d10d",
   "metadata": {},
   "source": [
    "## linear regression: predict crowding distance based on vmag_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {1:[],2:[],3:[],4:[]}\n",
    "for subject, ssdf in df_mean.groupby(['sid']):\n",
    "    if (subject_data['b']>5).any():\n",
    "        continue\n",
    "    for lbl in [1,2,3,4]:\n",
    "        sssdf = ssdf[ssdf['label'] == lbl]\n",
    "        x = sssdf['vmag1d_fit'].values\n",
    "        y = sssdf['CrowdingDistance'].values\n",
    "        (rss,coef) = cc.regression.fit_and_evaluate(x, y)\n",
    "        res[lbl].append(rss)\n",
    "            \n",
    "print({k:np.mean(v) for (k,v) in res.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {1:[],2:[],3:[],4:[]}\n",
    "for subject, subject_data in df_HM.groupby(['sid']):\n",
    "    if (subject_data['b']>5).any():\n",
    "        continue\n",
    "    for h in ['lh','rh']:\n",
    "        ssdf = subject_data[subject_data['h'] == h]\n",
    "        for lbl in [1,2,3,4]:\n",
    "            sssdf = ssdf[ssdf['label'] == lbl]\n",
    "            x = sssdf['vmag1d_fit'].values\n",
    "            y = sssdf['CrowdingDistance_HM'].values\n",
    "            (rss,coef) = cc.regression.fit_and_evaluate(x, y)\n",
    "            res[lbl].append(rss)\n",
    "            \n",
    "print({k:np.mean(v) for (k,v) in res.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a43647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = df_HM.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b']>5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "    for h in ['lh', 'rh']:\n",
    "        hemisphere_data = subject_data[subject_data['h'] == h]\n",
    "        \n",
    "        y1 = hemisphere_data[hemisphere_data['label'] == 1]['vmag1d_fit'].values\n",
    "        y2 = hemisphere_data[hemisphere_data['label'] == 2]['vmag1d_fit'].values\n",
    "        y3 = hemisphere_data[hemisphere_data['label'] == 3]['vmag1d_fit'].values\n",
    "        y4 = hemisphere_data[hemisphere_data['label'] == 4]['vmag1d_fit'].values\n",
    "        # since cd are the same for label 1,2,3,4\n",
    "        y5 = hemisphere_data[hemisphere_data['label'] == 1]['CrowdingDistance_HM'].values\n",
    "        \n",
    "        rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "        rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "        rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "        rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "        \n",
    "        rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "        best_fit_index = np.argmin(rss_values)\n",
    "        best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "        \n",
    "        subject_results[subject][h] = {\n",
    "            'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "            'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "            'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "            'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "            'best_fit': best_fit\n",
    "        }\n",
    "\n",
    "# for subject, hemispheres in subject_results.items():\n",
    "#     print(f\"Results for {subject}:\")\n",
    "#     for h, results in hemispheres.items():\n",
    "#         print(f\"  Hemisphere: {h}\")\n",
    "#         print(f\"    RSS for label 1: {results['rss_label_1']}, Coefficient: {results['coef_label_1']}\")\n",
    "#         print(f\"    RSS for label 2: {results['rss_label_2']}, Coefficient: {results['coef_label_2']}\")\n",
    "#         print(f\"    RSS for label 3: {results['rss_label_3']}, Coefficient: {results['coef_label_3']}\")\n",
    "#         print(f\"    RSS for label 4: {results['rss_label_4']}, Coefficient: {results['coef_label_4']}\")\n",
    "#         print(f\"    The best fit is: {results['best_fit']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50834602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rss_keys = ['rss_label_1', 'rss_label_2', 'rss_label_3', 'rss_label_4']\n",
    "\n",
    "# data = [[\n",
    "#     hemispheres[h][key]  \n",
    "#     for subject, hemispheres in subject_results.items()\n",
    "#     for h in ['lh', 'rh'] \n",
    "# ] for key in rss_keys]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(data, label=['Label 1', 'Label 2', 'Label 3', 'Label 4'])\n",
    "# plt.xlabel('RSS Values')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of RSS Values for Each Label')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d15397",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, hemispheres in subject_results.items():\n",
    "    for h in ['lh', 'rh']:\n",
    "        rss_label_1_values.append(hemispheres[h]['rss_label_1'])\n",
    "        rss_label_2_values.append(hemispheres[h]['rss_label_2'])\n",
    "        rss_label_3_values.append(hemispheres[h]['rss_label_3'])\n",
    "        rss_label_4_values.append(hemispheres[h]['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subject_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c26b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_label_1, mean_rss_label_2, mean_rss_label_3, mean_rss_label_4]\n",
    "std_rss = [std_rss_label_1, std_rss_label_2, std_rss_label_3, std_rss_label_4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# standard error of the mean (SEM)\n",
    "n_subjects = len(subject_results) * 2  # for both hemispheres\n",
    "sem_rss = np.array(std_rss) / np.sqrt(n_subjects)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=sem_rss)\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_VM.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b'] > 5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "\n",
    "    y1 = subject_data[subject_data['label'] == 1]['vmag1d_fit'].values\n",
    "    y2 = subject_data[subject_data['label'] == 2]['vmag1d_fit'].values\n",
    "    y3 = subject_data[subject_data['label'] == 3]['vmag1d_fit'].values\n",
    "    y4 = subject_data[subject_data['label'] == 4]['vmag1d_fit'].values\n",
    "    y5 = subject_data[subject_data['label'] == 1]['CrowdingDistance_LVM'].values \n",
    "\n",
    "    rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "    rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "    rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "    rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "\n",
    "    rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "    best_fit_index = np.argmin(rss_values)\n",
    "    best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "\n",
    "    subject_results[subject] = {\n",
    "        'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "        'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "        'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "        'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "        'best_fit': best_fit\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427245ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, results in subject_results.items():\n",
    "    rss_label_1_values.append(results['rss_label_1'])\n",
    "    rss_label_2_values.append(results['rss_label_2'])\n",
    "    rss_label_3_values.append(results['rss_label_3'])\n",
    "    rss_label_4_values.append(results['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_label_1, mean_rss_label_2, mean_rss_label_3, mean_rss_label_4]\n",
    "std_rss = [std_rss_label_1, std_rss_label_2, std_rss_label_3, std_rss_label_4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# standard error of the mean (SEM)\n",
    "n_subjects = len(subject_results)\n",
    "sem_rss = np.array(std_rss) / np.sqrt(n_subjects)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=sem_rss)\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dce51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_VM.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b'] > 5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "\n",
    "    y1 = subject_data[subject_data['label'] == 1]['vmag1d_fit'].values\n",
    "    y2 = subject_data[subject_data['label'] == 2]['vmag1d_fit'].values\n",
    "    y3 = subject_data[subject_data['label'] == 3]['vmag1d_fit'].values\n",
    "    y4 = subject_data[subject_data['label'] == 4]['vmag1d_fit'].values\n",
    "    y5 = subject_data[subject_data['label'] == 1]['CrowdingDistance_UVM'].values  # same for all labels\n",
    "\n",
    "    rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "    rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "    rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "    rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "\n",
    "    rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "    best_fit_index = np.argmin(rss_values)\n",
    "    best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "\n",
    "    subject_results[subject] = {\n",
    "        'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "        'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "        'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "        'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "        'best_fit': best_fit\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, results in subject_results.items():\n",
    "    rss_label_1_values.append(results['rss_label_1'])\n",
    "    rss_label_2_values.append(results['rss_label_2'])\n",
    "    rss_label_3_values.append(results['rss_label_3'])\n",
    "    rss_label_4_values.append(results['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6aaebc",
   "metadata": {},
   "source": [
    "### df_mean result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49190448",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_mean.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b'] > 5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "\n",
    "    y1 = subject_data[subject_data['label'] == 1]['vmag1d_fit'].values\n",
    "    y2 = subject_data[subject_data['label'] == 2]['vmag1d_fit'].values\n",
    "    y3 = subject_data[subject_data['label'] == 3]['vmag1d_fit'].values\n",
    "    y4 = subject_data[subject_data['label'] == 4]['vmag1d_fit'].values\n",
    "    y5 = subject_data[subject_data['label'] == 1]['CrowdingDistance'].values \n",
    "\n",
    "    rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "    rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "    rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "    rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "\n",
    "    rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "    best_fit_index = np.argmin(rss_values)\n",
    "    best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "\n",
    "    subject_results[subject] = {\n",
    "        'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "        'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "        'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "        'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "        'best_fit': best_fit\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, results in subject_results.items():\n",
    "    rss_label_1_values.append(results['rss_label_1'])\n",
    "    rss_label_2_values.append(results['rss_label_2'])\n",
    "    rss_label_3_values.append(results['rss_label_3'])\n",
    "    rss_label_4_values.append(results['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_label_1, mean_rss_label_2, mean_rss_label_3, mean_rss_label_4]\n",
    "std_rss = [std_rss_label_1, std_rss_label_2, std_rss_label_3, std_rss_label_4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# standard error of the mean (SEM)\n",
    "n_subjects = len(subject_results)  #15\n",
    "sem_rss = np.array(std_rss) / np.sqrt(n_subjects)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=sem_rss)\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd6bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00c26ff4",
   "metadata": {},
   "source": [
    "# previous code used in VSS24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacd631",
   "metadata": {},
   "source": [
    "### bootstrap crowding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_cd(x, b):\n",
    "    return np.log10((0.43 + x + 0.06*(x**2)) * b)\n",
    "\n",
    "# the number of bootstrap samples\n",
    "num_bootstrap_samples = 1000\n",
    "x = np.linspace(0.5,10,1000)\n",
    "eccentricities = [2.5, 5, 10]\n",
    "\n",
    "# sid_df.shape=(480,)\n",
    "sid_df = df['Observer'].values\n",
    "x_ecc = np.array(x_ecc)\n",
    "cd = np.array(cd_list)\n",
    "\n",
    "def bootstrap_fit(sids, xdata, ydata, x):\n",
    "    # unique_sids : 20 numbers\n",
    "    unique_sids = np.unique(sids)\n",
    "    bootstrapped_parameters = []\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        # each bootstrap, sample 20 subjects with replacement\n",
    "        indices = np.random.choice(unique_sids, size=len(unique_sids), replace=True)\n",
    "        indices = [np.where(sids == sid)[0] for sid in indices]\n",
    "        indices = [k for ak in indices for k in ak]\n",
    "        # 20 by 24 = 480, 480 x values and y values each\n",
    "        x_boot = xdata[indices]\n",
    "        y_boot = ydata[indices]\n",
    "        # Fit the curve to the bootstrapped sample\n",
    "        b, _ = curve_fit(func_cd, x_boot, np.log10(y_boot), p0=0.15)\n",
    "        y = (0.43 + x + 0.06*(x**2)) * b\n",
    "        bootstrapped_parameters.append(y) \n",
    "    return bootstrapped_parameters\n",
    "\n",
    "bootstrapped = bootstrap_fit(sid_df, x_ecc, cd, x)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval_cd = np.percentile(bootstrapped, [2.5, 97.5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 10, 1000)\n",
    "# Fitted value without bootstrap\n",
    "plt.plot(x, (0.43 + x + 0.06*(x**2)) * b, 'k-', label='Fitted Crowding Distance')\n",
    "\n",
    "# Plot individual data\n",
    "plt.plot(mean_x_ecc, mean_cd_list, 'ko', alpha=0.1, label='Individual Crowding Distance')\n",
    "\n",
    "# Plot error bars\n",
    "plt.errorbar(eccentricities, mean_values, yerr=std_values, fmt='o', color='red', label='Mean ± Std')\n",
    "plt.fill_between(x, confidence_interval_cd[0], confidence_interval_cd[1], color='gray', alpha=0.3, label='95% Confidence Interval')\n",
    "\n",
    "plt.xlabel('Eccentricity (deg)')\n",
    "plt.ylabel('Crowding distance (deg)')\n",
    "plt.yscale('log')\n",
    "plt.ylim(bottom=0.1)  # Set lower limit to 0.1 (10^-1)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12b53f",
   "metadata": {},
   "source": [
    "### bootstrap C.Mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec282a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cmag\n",
    "all_cmag_v1 = []\n",
    "all_cmag_v2 = []\n",
    "all_cmag_v3 = []\n",
    "all_cmag_v4 = []\n",
    "all_eccen_v1 = []\n",
    "all_eccen_v2 = []\n",
    "all_eccen_v3 = []\n",
    "all_eccen_v4 = []\n",
    "eccen = np.linspace(1, 11, 1000)\n",
    "subjects_added = []  \n",
    "all_mask = ('variance_explained', 0.04, 1)\n",
    "\n",
    "for sid in sids:\n",
    "    try:\n",
    "        sub = load_subject(sid)\n",
    "\n",
    "        # Calculate cmag for the subject for V1\n",
    "        v1_mask = {'and': [('visual_area', 1), all_mask]}\n",
    "        eccen_v1, cmag_v1 = ring_cmag(sub, eccen=None, mask=v1_mask)\n",
    "        all_eccen_v1.append(eccen_v1)\n",
    "        all_cmag_v1.append(cmag_v1)\n",
    "\n",
    "        # Calculate cmag for the subject for V2\n",
    "        v2_mask = {'and': [('visual_area', 2), all_mask]}\n",
    "        eccen_v2, cmag_v2 = ring_cmag(sub, eccen=None, mask=v2_mask)\n",
    "        all_eccen_v2.append(eccen_v2)\n",
    "        all_cmag_v2.append(cmag_v2)\n",
    "\n",
    "        # Calculate cmag for the subject for V3\n",
    "        v3_mask = {'and': [('visual_area', 3), all_mask]}\n",
    "        eccen_v3, cmag_v3 = ring_cmag(sub, eccen=None, mask=v3_mask)\n",
    "        all_eccen_v3.append(eccen_v3)\n",
    "        all_cmag_v3.append(cmag_v3)\n",
    "        \n",
    "        # Calculate cmag for the subject for V4\n",
    "        v4_mask = {'and': [('visual_area', 4), all_mask]}\n",
    "        eccen_v4, cmag_v4 = ring_cmag(sub, eccen=None, mask=v4_mask)\n",
    "        all_eccen_v4.append(eccen_v4)\n",
    "        all_cmag_v4.append(cmag_v4)\n",
    "        \n",
    "        subjects_added.append(sid)  # Add subject to the list of subjects added\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating cmag for subject {sid}: {e}\")\n",
    "\n",
    "\n",
    "# Convert lists to arrays\n",
    "# all_cmag_v1 = np.array(all_cmag_v1)\n",
    "# all_cmag_v2 = np.array(all_cmag_v2)\n",
    "# all_cmag_v3 = np.array(all_cmag_v3)\n",
    "# all_cmag_v4: len=35, each array has diff shape\n",
    "all_flatcmag_v4 = np.concatenate(all_cmag_v4)\n",
    "all_flateccen_v4 = np.concatenate(all_eccen_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen = np.linspace(1,11, 1000)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return (a / (b + x))**2\n",
    "# use all_cmag_v1 contains cmag for v1 for 29 subjects\n",
    "subjects_added = np.array(subjects_added)\n",
    "x_data = np.array(eccen)\n",
    "cmag_v1 = np.array(all_cmag_v1)\n",
    "cmag_v2 = np.array(all_cmag_v2)\n",
    "cmag_v3 = np.array(all_cmag_v3)\n",
    "cmag_v4 = np.array(all_cmag_v4)\n",
    "\n",
    "def bootstrap_fit_cmag(sids, xdata, ydata, x, p0):\n",
    "    unique_sids = np.unique(sids)\n",
    "    bootstrapped_parameters = []\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        # Sample subjects\n",
    "        indices = np.random.choice(unique_sids, size=len(unique_sids), replace=True)\n",
    "        indices = [np.where(sids == sid)[0] for sid in indices]\n",
    "        indices = [k for ak in indices for k in ak]\n",
    "        x_boot = xdata[indices]\n",
    "        y_boot = ydata[indices]\n",
    "        # Fit the curve to the bootstrapped sample\n",
    "        popt, _ = curve_fit(func, x_boot.flatten(), y_boot.flatten(),p0=p0)\n",
    "        # store the function value\n",
    "        y = (popt[0] / (popt[1] + x))**2\n",
    "        bootstrapped_parameters.append(y) \n",
    "    return bootstrapped_parameters\n",
    "\n",
    "all_eccen = np.array([x_data]*len(subjects_added))\n",
    "\n",
    "bootstrapped_v1 = bootstrap_fit_cmag(subjects_added, all_eccen, cmag_v1, eccen, p0=[17.3, 0.75])\n",
    "bootstrapped_v2 = bootstrap_fit_cmag(subjects_added, all_eccen, cmag_v2, eccen, p0=[17.3, 0.75])\n",
    "bootstrapped_v3 = bootstrap_fit_cmag(subjects_added, all_eccen, cmag_v3, eccen, p0=[17.3, 0.75])\n",
    "bootstrapped_v4 = bootstrap_fit_cmag(subjects_added, all_eccen, cmag_v4, eccen, p0=[17.3, 0.75])\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval_v1 = np.percentile(bootstrapped_v1, [2.5, 97.5], axis=0)\n",
    "confidence_interval_v2 = np.percentile(bootstrapped_v2, [2.5, 97.5], axis=0)\n",
    "confidence_interval_v3 = np.percentile(bootstrapped_v3, [2.5, 97.5], axis=0)\n",
    "confidence_interval_v4 = np.percentile(bootstrapped_v4, [2.5, 97.5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb460ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the average data for each visual area\n",
    "ax.plot(eccen, np.sqrt(average_cmag_v1/2), 'k:', label='V1')\n",
    "ax.plot(eccen, np.sqrt(average_cmag_v2/2), 'r:', label='V2')\n",
    "ax.plot(eccen, np.sqrt(average_cmag_v3/2), 'm:', label='V3')\n",
    "ax.plot(eccen, np.sqrt(average_cmag_v4/2), 'g:', label='hV4')\n",
    "\n",
    "# Plotting the fitted lines for each visual area\n",
    "ax.plot(eccen, np.sqrt((popt1[0]/(eccen+popt1[1]))**2/2), 'k', label='V1 fitted line')\n",
    "ax.plot(eccen, np.sqrt((popt2[0]/(eccen+popt2[1]))**2/2), 'r', label='V2 fitted line')\n",
    "ax.plot(eccen, np.sqrt((popt3[0]/(eccen+popt3[1]))**2/2), 'm', label='V3 fitted line')\n",
    "ax.plot(eccen, np.sqrt((popt4[0]/(eccen+popt4[1]))**2/2), 'g', label='hV4 fitted line')\n",
    "\n",
    "# Plotting the confidence intervals for each visual area\n",
    "ax.fill_between(eccen, \n",
    "                 np.sqrt(confidence_interval_v1[0]/2),\n",
    "                 np.sqrt(confidence_interval_v1[1]/2),\n",
    "                 color='k', alpha=0.3)\n",
    "ax.fill_between(eccen, \n",
    "                 np.sqrt(confidence_interval_v2[0]/2),\n",
    "                 np.sqrt(confidence_interval_v2[1]/2),\n",
    "                 color='r', alpha=0.3)\n",
    "ax.fill_between(eccen, \n",
    "                 np.sqrt(confidence_interval_v3[0]/2),\n",
    "                 np.sqrt(confidence_interval_v3[1]/2),\n",
    "                 color='m', alpha=0.3)\n",
    "ax.fill_between(eccen, \n",
    "                 np.sqrt(confidence_interval_v4[0]/2),\n",
    "                 np.sqrt(confidence_interval_v4[1]/2),\n",
    "                 color='g', alpha=0.3)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Eccentricity (deg)\")\n",
    "ax.set_ylabel(\"Radial Cortical Magnification (mm/deg)\")\n",
    "ax.set_title(\"Average Radial Cortical Magnification for V1, V2, V3, hV4\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8980017",
   "metadata": {},
   "source": [
    "### using bootstrapped fits to get cortical crowding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store bootstrapped CCD values for each visual area\n",
    "bootstrapped_ccd_1 = []\n",
    "bootstrapped_ccd_2 = []\n",
    "bootstrapped_ccd_3 = []\n",
    "bootstrapped_ccd_4 = []\n",
    "\n",
    "# \"bootstrapped\" here refers to crowding distance result\n",
    "for i in range(len(bootstrapped)):\n",
    "    # Calculate bootstrapped CCD for visual area 1\n",
    "    ccd_v1 = bootstrapped[i] * np.sqrt(bootstrapped_v1[i] / 2)\n",
    "    bootstrapped_ccd_1.append(ccd_v1)\n",
    "    \n",
    "    # Calculate bootstrapped CCD for visual area 2\n",
    "    ccd_v2 = bootstrapped[i] * np.sqrt(bootstrapped_v2[i] / 2)\n",
    "    bootstrapped_ccd_2.append(ccd_v2)\n",
    "    \n",
    "    # Calculate bootstrapped CCD for visual area 3\n",
    "    ccd_v3 = bootstrapped[i] * np.sqrt(bootstrapped_v3[i] / 2)\n",
    "    bootstrapped_ccd_3.append(ccd_v3)\n",
    "    \n",
    "    # Calculate bootstrapped CCD for visual area 4\n",
    "    ccd_v4 = bootstrapped[i] * np.sqrt(bootstrapped_v4[i] / 2)\n",
    "    bootstrapped_ccd_4.append(ccd_v4)\n",
    "\n",
    "# Convert lists to arrays\n",
    "bootstrapped_ccd_1 = np.array(bootstrapped_ccd_1)\n",
    "bootstrapped_ccd_2 = np.array(bootstrapped_ccd_2)\n",
    "bootstrapped_ccd_3 = np.array(bootstrapped_ccd_3)\n",
    "bootstrapped_ccd_4 = np.array(bootstrapped_ccd_4)\n",
    "\n",
    "# Calculate the mean of bootstrapped CCD values for each visual area\n",
    "xx1 = np.mean(bootstrapped_ccd_1, axis=0)\n",
    "xx2 = np.mean(bootstrapped_ccd_2, axis=0)\n",
    "xx3 = np.mean(bootstrapped_ccd_3, axis=0)\n",
    "xx4 = np.mean(bootstrapped_ccd_4, axis=0)\n",
    "\n",
    "# Calculate the confidence interval for bootstrapped CCD values for each visual area\n",
    "confidence_interval_ccd_1 = np.percentile(bootstrapped_ccd_1,  [16, 84], axis=0)\n",
    "confidence_interval_ccd_2 = np.percentile(bootstrapped_ccd_2,  [16, 84], axis=0)\n",
    "confidence_interval_ccd_3 = np.percentile(bootstrapped_ccd_3,  [16, 84], axis=0)\n",
    "confidence_interval_ccd_4 = np.percentile(bootstrapped_ccd_4,  [16, 84], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74cf81",
   "metadata": {},
   "source": [
    "### coefficient of variation for cortical crowding distance at each area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ff518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coefficient of variation for ccd_1\n",
    "mean_ccd_1 = np.mean(xx1)\n",
    "std_ccd_1 = np.std(xx1)\n",
    "cv_ccd_1 = std_ccd_1 / mean_ccd_1\n",
    "rounded_cv_ccd_1 = round(cv_ccd_1, 2)\n",
    "\n",
    "print(\"Coefficient of Variation (CCD 1):\", rounded_cv_ccd_1)\n",
    "\n",
    "# Calculate the coefficient of variation for ccd_2\n",
    "mean_ccd_2 = np.mean(xx2)\n",
    "std_ccd_2 = np.std(xx2)\n",
    "cv_ccd_2 = std_ccd_2 / mean_ccd_2\n",
    "rounded_cv_ccd_2 = round(cv_ccd_2, 2)\n",
    "print(\"Coefficient of Variation (CCD 2):\", rounded_cv_ccd_2)\n",
    "\n",
    "# Calculate the coefficient of variation for ccd_3\n",
    "mean_ccd_3 = np.mean(xx3)\n",
    "std_ccd_3 = np.std(xx3)\n",
    "cv_ccd_3 = std_ccd_3 / mean_ccd_3\n",
    "rounded_cv_ccd_3 = round(cv_ccd_3, 2)\n",
    "print(\"Coefficient of Variation (CCD 3):\", rounded_cv_ccd_3)\n",
    "\n",
    "# Calculate the coefficient of variation for ccd_4\n",
    "mean_ccd_4 = np.mean(xx4)\n",
    "std_ccd_4 = np.std(xx4)\n",
    "cv_ccd_4 = std_ccd_4 / mean_ccd_4\n",
    "rounded_cv_ccd_4 = round(cv_ccd_4, 2)\n",
    "print(\"Coefficient of Variation (CCD 4):\", rounded_cv_ccd_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_cv(data, num_samples):\n",
    "    \"\"\"\n",
    "    Perform bootstrapping to compute the coefficient of variation (CV).\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray)\n",
    "        num_samples (int): Number of bootstrap samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array containing the bootstrapped CV values.\n",
    "    \"\"\"\n",
    "    bootstrapped_cv = []\n",
    "    n = len(data)\n",
    "    for _ in range(num_samples):\n",
    "        sample_indices = np.random.choice(range(n), size=n, replace=True)\n",
    "        bootstrapped_sample = data[sample_indices]\n",
    "        mean_sample = np.mean(bootstrapped_sample)\n",
    "        std_sample = np.std(bootstrapped_sample)\n",
    "        cv_sample = std_sample / mean_sample\n",
    "        bootstrapped_cv.append(cv_sample)\n",
    "    return np.array(bootstrapped_cv)\n",
    "\n",
    "# Perform bootstrap on CCD 1\n",
    "bootstrapped_cv_1 = bootstrap_cv(xx1, num_samples=1000)\n",
    "ci_1 = np.percentile(bootstrapped_cv_1, [2.5, 97.5])\n",
    "\n",
    "# Perform bootstrap on CCD 2\n",
    "bootstrapped_cv_2 = bootstrap_cv(xx2, num_samples=1000)\n",
    "ci_2 = np.percentile(bootstrapped_cv_2, [2.5, 97.5])\n",
    "\n",
    "# Perform bootstrap on CCD 3\n",
    "bootstrapped_cv_3 = bootstrap_cv(xx3, num_samples=1000)\n",
    "ci_3 = np.percentile(bootstrapped_cv_3, [2.5, 97.5])\n",
    "\n",
    "# Perform bootstrap on CCD 4\n",
    "bootstrapped_cv_4 = bootstrap_cv(xx4, num_samples=1000)\n",
    "ci_4 = np.percentile(bootstrapped_cv_4, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a62301",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ccd_1 = bootstrapped_cv_1.mean()\n",
    "cv_ccd_2 = bootstrapped_cv_2.mean()\n",
    "cv_ccd_3 = bootstrapped_cv_3.mean()\n",
    "cv_ccd_4 = bootstrapped_cv_4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47861a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of mean CV in each area\n",
    "cv_ccd_values = [cv_ccd_1, cv_ccd_2, cv_ccd_3, cv_ccd_4]\n",
    "ccd_labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# a list of CI in each area\n",
    "cv_ci_list = [(ci_1[0], ci_1[1]), (ci_2[0], ci_2[1]), (ci_3[0], ci_3[1]),(ci_4[0], ci_4[1])]\n",
    "\n",
    "# lower and upper bounds of CI\n",
    "lower_bound = [ci[0] for ci in cv_ci_list]\n",
    "upper_bound = [ci[1] for ci in cv_ci_list]\n",
    "\n",
    "yerr = [[cv_ccd_values[i] - lower_bound[i] for i in range(len(cv_ccd_values))],\n",
    "        [upper_bound[i] - cv_ccd_values[i] for i in range(len(cv_ccd_values))]]\n",
    "\n",
    "# bar plot with error bars\n",
    "plt.bar(ccd_labels, cv_ccd_values, yerr=yerr, capsize=5, color=['grey', 'red', 'magenta', 'green'])\n",
    "\n",
    "plt.xlabel('Visual Areas')\n",
    "plt.ylabel('Coefficient of Variation')\n",
    "plt.title('Coefficient of Variation for Cortical Crowding Distance')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cortical crowding distance vs eccen\n",
    "x = np.linspace(0.5, 10, 1000)\n",
    "plt.plot(x, xx1, label='Cortical crowding distance in V1', color='black')\n",
    "plt.plot(x, xx2, label='Cortical crowding distance in V2', color='red')\n",
    "plt.plot(x, xx3, label='Cortical crowding distance in V3', color='magenta')\n",
    "plt.plot(x, xx4, label='Cortical crowding distance in hV4', color='green')\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(x, confidence_interval_ccd_1[0], confidence_interval_ccd_1[1], color='black', alpha=0.3)\n",
    "plt.fill_between(x, confidence_interval_ccd_2[0], confidence_interval_ccd_2[1], color='red', alpha=0.3)\n",
    "plt.fill_between(x, confidence_interval_ccd_3[0], confidence_interval_ccd_3[1], color='magenta', alpha=0.3)\n",
    "plt.fill_between(x, confidence_interval_ccd_4[0], confidence_interval_ccd_4[1], color='green', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Eccentricity (deg)')\n",
    "plt.ylabel('Cortical Crowding Distance (mm)')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
