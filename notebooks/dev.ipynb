{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7af612-e71d-4dfc-a477-088a7f79c1b6",
   "metadata": {},
   "source": [
    "# Development Notebook for Cortical Crowding Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8a076-56ce-45b0-950d-b70220b4d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuropythy as ny\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize\n",
    "from scipy.stats import gmean\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328aa4ac-48d3-46ba-8748-e8c287d3edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to be able to load in libraries that are in this repository's src directory,\n",
    "# so we add src to the system path:\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Now we can import corticalcrowding from the src directory:\n",
    "import corticalcrowding as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/crowding/crowding_data_withID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838bf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids_NYU = [\n",
    "    'sub-wlsubj070',\n",
    "    'sub-wlsubj114',\n",
    "    'sub-wlsubj121',\n",
    "    'sub-wlsubj135']\n",
    "sids_NEI = [ \n",
    "    'sub-wlsubj119',\n",
    "    'sub-wlsubj127',\n",
    "    'sub-wlsubj136',\n",
    "    'sub-wlsubj137',\n",
    "    'sub-wlsubj143',\n",
    "    'sub-wlsubj144',\n",
    "    'sub-wlsubj145',\n",
    "    'sub-wlsubj146',\n",
    "    'sub-wlsubj147',\n",
    "    'sub-wlsubj148',\n",
    "    'sub-wlsubj149',\n",
    "    'sub-wlsubj150',\n",
    "    'sub-wlsubj151',\n",
    "    'sub-wlsubj152',\n",
    "    'sub-wlsubj153',\n",
    "    'sub-wlsubj154',\n",
    "    'sub-wlsubj155',\n",
    "    'sub-wlsubj156',\n",
    "    'sub-wlsubj157',\n",
    "    'sub-wlsubj158',\n",
    "    'sub-wlsubj159',\n",
    "    'sub-wlsubj160',\n",
    "    'sub-wlsubj161',\n",
    "    'sub-wlsubj162',\n",
    "    'sub-wlsubj163',\n",
    "    'sub-wlsubj164',\n",
    "    'sub-wlsubj165',\n",
    "    'sub-wlsubj166',\n",
    "    'sub-wlsubj167',\n",
    "    'sub-wlsubj168',\n",
    "    'sub-wlsubj170',\n",
    "    'sub-wlsubj171',\n",
    "    'sub-wlsubj172',\n",
    "    'sub-wlsubj173',\n",
    "    'sub-wlsubj174',\n",
    "    'sub-wlsubj175',\n",
    "    'sub-wlsubj176']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids_orig = sids_NYU + sids_NEI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c2085",
   "metadata": {},
   "source": [
    "## crowding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 sessions, so now each sub have 12 cd (match with polar angle) instead of 24\n",
    "mean_cd_polar = df.groupby(['ID','Eccen_X','Eccen_Y'])['CrowdingDistance'].apply(gmean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each subject has 1 cd value at each eccentricity\n",
    "mean_cd = df.groupby(['ID','RadialEccentricity'])['CrowdingDistance'].apply(gmean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_list = df['CrowdingDistance'].tolist()\n",
    "mean_cd_list=mean_cd['CrowdingDistance'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 dfs based on eccen\n",
    "mean_1 = mean_cd[mean_cd['RadialEccentricity']==2.5]\n",
    "m_1 = mean_1['CrowdingDistance'].mean()\n",
    "st_1 = mean_1['CrowdingDistance'].std()\n",
    "mean_2 = mean_cd[mean_cd['RadialEccentricity']==5]\n",
    "m_2 = mean_2['CrowdingDistance'].mean()\n",
    "st_2 = mean_2['CrowdingDistance'].std()\n",
    "mean_3 = mean_cd[mean_cd['RadialEccentricity']==10]\n",
    "m_3 = mean_3['CrowdingDistance'].mean()\n",
    "st_3 = mean_3['CrowdingDistance'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ecc = df['RadialEccentricity'].tolist()\n",
    "mean_x_ecc = mean_cd['RadialEccentricity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, _ = curve_fit(cc.crowding.func_cd, x_ecc, np.log10(cd_list), p0=0.15)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21810d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = [m_1, m_2, m_3]\n",
    "std_values = [st_1, st_2, st_3]\n",
    "eccentricities = [2.5, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431553e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0c570",
   "metadata": {},
   "source": [
    "## fit cortical magnification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen = np.linspace(1, 11, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b663011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmag_basics(sid, h, label):\n",
    "    sub = cc.cmag.load_subject(sid)\n",
    "    hem = sub.hemis[h]\n",
    "    mask_nor2 = {\n",
    "        'and': [\n",
    "            ('eccentricity', 0, 12),\n",
    "            ('visual_area', label)]}\n",
    "    mask_r2 = {'and': mask_nor2['and'] + [('variance_explained', 0.04, 1)]}\n",
    "    rdat = ny.retinotopy_data(hem)\n",
    "    mask_ii = hem.mask(mask_r2)\n",
    "    ecc = rdat['eccentricity'][mask_ii]\n",
    "    srf = hem.prop('midgray_surface_area')\n",
    "    totarea = np.sum(srf[hem.mask(mask_nor2)])\n",
    "    srf = srf[mask_ii]\n",
    "    ii = np.argsort(ecc)\n",
    "    ecc = ecc[ii]\n",
    "    srf = srf[ii] * totarea / np.sum(srf)\n",
    "    return (ecc, srf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b45f72",
   "metadata": {},
   "source": [
    "$$ c_1 = \\sqrt{\\frac{a_M}{\\pi \\left(\\log\\left(\\frac{c_2 + M}{c_2}\\right) - \\frac{M}{c_2 + M}\\right)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8127d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HH91_integral(x, a=17.3, b=0.75):\n",
    "    xb = x + b\n",
    "    return a**2 * np.pi * (np.log(xb / b) - x / xb)\n",
    "\n",
    "def HH91_c1(totalarea, maxecc, b=0.75):\n",
    "    mb = maxecc + b\n",
    "    c1 = np.sqrt(totalarea / np.pi / (np.log(mb / b) - maxecc/mb))\n",
    "\n",
    "def fit_cumarea_data(ecc, srf, params0=(17.3, 0.75), method=None):\n",
    "    from scipy.optimize import minimize\n",
    "    ecc = np.asarray(ecc, dtype=np.float64)\n",
    "    srf = np.asarray(srf, dtype=np.float64)\n",
    "    ii = np.argsort(ecc)\n",
    "    ecc = ecc[ii]\n",
    "    srf = srf[ii]\n",
    "    cumsrf = np.cumsum(srf)\n",
    "    def loss_vmag(params):\n",
    "        params = list(params)\n",
    "        params[1] = params[1]**2\n",
    "        pred = HH91_integral(ecc, *params)\n",
    "        error = (pred - cumsrf)\n",
    "        return np.mean(error**2)\n",
    "    params0 = list(params0)\n",
    "    params0[1] = np.sqrt(params0[1])\n",
    "    r = minimize(loss_vmag, params0, method=method)\n",
    "    r.x[0] = abs(r.x[0])\n",
    "    r.x[1] = r.x[1]**2\n",
    "    r.coords = np.array([ecc,srf])\n",
    "    return r\n",
    "\n",
    "def fit_cumarea(sid, h, label):\n",
    "    (ecc,srf) = cmag_basics(sid, h, label)\n",
    "    if len(ecc) == 0:\n",
    "        raise RuntimeError(f\"no data found for {sid}:{h}:{label}\")\n",
    "    r = fit_cumarea_data(ecc, srf)\n",
    "    r.coords = np.array([ecc, srf])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a52e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dict(sid=[], h=[], label=[], a=[], b=[], loss=[])\n",
    "for sid in sids_orig:\n",
    "    print(sid)\n",
    "    for h in ['lh','rh']:\n",
    "        for lbl in [1,2,3,4]:\n",
    "            try:\n",
    "                r = fit_cumarea(sid, h, lbl)\n",
    "            except Exception as e:\n",
    "                print(f\"  - Skipping: {type(e)}\")\n",
    "                continue\n",
    "            df['sid'].append(sid)\n",
    "            df['h'].append(h)\n",
    "            df['label'].append(lbl)\n",
    "            df['a'].append(r.x[0])\n",
    "            df['b'].append(r.x[1])\n",
    "            df['loss'].append(r.fun)\n",
    "HH91_params = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdb4b6",
   "metadata": {},
   "source": [
    "### histogram of \"a\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        q = df2['a'].values\n",
    "        ax.hist(q)\n",
    "        ax.set_xlim([0,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ca7d",
   "metadata": {},
   "source": [
    "### histogram of \"b\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        q = df2['b'].values\n",
    "        ax.hist(q)\n",
    "        #ax.set_xlim([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ee13e",
   "metadata": {},
   "source": [
    "### plot hh91 fit vs eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(0,0,1,1,0.1,0.1)\n",
    "eccrng = np.linspace(0.5, 11, 500)\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        for (a,b) in zip(df2['a'].values, df2['b'].values):\n",
    "            ax.loglog(eccrng, (a / (b + eccrng))**2, 'k-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params\n",
    "(fig,axs) = plt.subplots(1,2, figsize=(4,1.5), dpi=288, sharex=True, sharey=True)\n",
    "eccrng = np.linspace(0.5, 12, 500)\n",
    "for (lbl,clr) in zip([1,2,3,4], ['r','g','b','k']):\n",
    "    df1 = df[df['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axs):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        a = df2['a'].values\n",
    "        b = df2['b'].values\n",
    "        m = (a[:,None] / (b[:,None] + eccrng[None,:]))**2\n",
    "        mu = np.mean(m, axis=0)\n",
    "        sd = np.std(m, axis=0)\n",
    "        ax.fill_between(eccrng, mu - sd, mu + sd, edgecolor=None, facecolor=clr, alpha=0.2, zorder=-1)\n",
    "        ax.loglog(eccrng, mu, clr+'-', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c59fbd",
   "metadata": {},
   "source": [
    "## have two dfs of fitted cortical magnification and crowding values based on VM/HM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8349c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 sessions, so now each sub have 12 cd (match with polar angle) instead of 24\n",
    "# mean_cd_polar = df.groupby(['ID','Eccen_X','Eccen_Y'])['CrowdingDistance'].apply(gmean).reset_index()\n",
    "mean_cd_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at horizontal meridian for tmp_hor\n",
    "# h is assigned 'lh' for x values larger than 0 and 'rh' otherwise\n",
    "tmp_hor = mean_cd_polar[mean_cd_polar['Eccen_Y']==0]\n",
    "x = tmp_hor['Eccen_X'].values\n",
    "# for calculating cmag, have a col for abs(x)\n",
    "tmp_hor['eccen'] = np.abs(x)\n",
    "tmp_hor['h'] = np.where(x > 0, 'lh', 'rh')\n",
    "tmp_hor.rename(columns={'CrowdingDistance': 'CrowdingDistance_HM'}, inplace=True)\n",
    "tmp_hor.rename(columns={'ID': 'sid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at vertical meridian for tmp_ver\n",
    "tmp_ver = mean_cd_polar[mean_cd_polar['Eccen_X']==0]\n",
    "y = tmp_ver['Eccen_Y'].values\n",
    "tmp_ver['eccen'] = np.abs(y)\n",
    "\n",
    "# Create the 'CrowdingDistance_UVM' and 'CrowdingDistance_LVM' columns\n",
    "tmp_ver['CrowdingDistance_UVM'] = np.where(y > 0, np.nan, tmp_ver['CrowdingDistance'])\n",
    "tmp_ver['CrowdingDistance_LVM'] = np.where(y > 0, tmp_ver['CrowdingDistance'], np.nan)\n",
    "tmp_ver.rename(columns={'ID': 'sid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be74eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create duplicate rows with 'h' column\n",
    "tmp_ver_lh = tmp_ver.copy()\n",
    "tmp_ver_lh['h'] = 'lh'\n",
    "tmp_ver_rh = tmp_ver.copy()\n",
    "tmp_ver_rh['h'] = 'rh'\n",
    "\n",
    "# Combine the two DataFrames, so now tmp_ver has two rows with same data except one row lh and one row rh\n",
    "tmp_ver = pd.concat([tmp_ver_lh, tmp_ver_rh], ignore_index=True)\n",
    "tmp_ver = tmp_ver.drop(columns=['CrowdingDistance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee551629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = HH91_params.copy()\n",
    "\n",
    "# will have three rows for diff ecc, but the same a,b params\n",
    "df = df.merge(\n",
    "    pd.DataFrame(dict(eccen=[2.5, 5.0, 10.0])),\n",
    "    how='cross')\n",
    "\n",
    "a = df['a']\n",
    "b = df['b']\n",
    "ecc = df['eccen']\n",
    "# calculate cmag based on a,b params from HH91_params\n",
    "df['cmag_fit'] = (a / (ecc + b))**2\n",
    "# add 1d visual magnification\n",
    "df['1d_vmag_fit'] = np.sqrt(1/ (a / (ecc + b))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add CrowdingDistance_HM to df by merging it with tmp_hor\n",
    "# _full versions have x, y positions too\n",
    "df_HM_full = df.merge(tmp_hor, on=['sid', 'eccen','h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce892b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM = df_HM_full.drop(columns=['Eccen_X', 'Eccen_Y']) # 15 subs, 2 hemisphere * 3 ecc * 4 visual area = 360 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM_full = df.merge(tmp_ver, on=['sid', 'eccen','h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e809e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM = df_VM_full.drop(columns=['Eccen_X', 'Eccen_Y']) #15 subs, 2 loc * 3 ecc * 4 visual area * 2h = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM = df_VM.groupby(\n",
    "    ['sid', 'label', 'eccen'], as_index=False\n",
    ").agg({\n",
    "    'a': 'mean',\n",
    "    'b': 'mean',\n",
    "    'loss': 'mean',\n",
    "    'CrowdingDistance_UVM': np.nanmean, \n",
    "    'CrowdingDistance_LVM': np.nanmean, \n",
    "})\n",
    "\n",
    "# compute cmag_fit based on a, b, and eccen\n",
    "df_VM['cmag_fit'] = (df_VM['a'] / (df_VM['eccen'] + df_VM['b']))**2\n",
    "\n",
    "# compute 1d_vmag_fit from cmag_fit\n",
    "df_VM['1d_vmag_fit'] = np.sqrt(1 / df_VM['cmag_fit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd354205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3118c2",
   "metadata": {},
   "source": [
    "# for each subject, each visual area, to have a fitted cortical crowding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_loss_notused(params, cd, cmags, position):\n",
    "    '''\n",
    "    position: 1 for horizontal, 2 for upper, 3 for lower\n",
    "    params: d for cortical crowding distance, g_h, g_u, g_l\n",
    "    cd: crowding distance\n",
    "    cmags: cortical magnification\n",
    "    '''\n",
    "    gain = params[position] ** 2  # ensure that gain >= 0\n",
    "    d = params[0]\n",
    "    pred = d * np.sqrt(2/(gain*cmags))\n",
    "    \n",
    "    return np.sum((cd - pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferred for now\n",
    "def crowding_loss(params, cd, cmags, position):\n",
    "    '''\n",
    "    position: 1 for horizontal, 2 for upper, 3 for lower\n",
    "    params: d, g_h, g_u, g_l\n",
    "    cd: crowding distance\n",
    "    cmags: cortical magnification\n",
    "    '''\n",
    "    gain = np.exp(params[position])\n",
    "    d = params[0]\n",
    "    pred = d * np.sqrt(2/(gain*cmags))\n",
    "    \n",
    "    return np.sum((cd - pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_fit_data(df_HM, df_VM, sid, label):\n",
    "    sub_hm = df_HM[(df_HM['label']==label) & (df_HM['sid']==sid)]\n",
    "    cd_hm = sub_hm['CrowdingDistance_HM'].values\n",
    "    cm_hm = sub_hm['cmag_fit'].values\n",
    "    pos_hm = np.ones(len(cd_hm),dtype=int)\n",
    "    \n",
    "    sub_vm = df_VM[(df_VM['label']==label) & (df_VM['sid']==sid)]\n",
    "    cd_um = sub_vm['CrowdingDistance_UVM'].values\n",
    "    cd_lm = sub_vm['CrowdingDistance_LVM'].values\n",
    "    cm_um = sub_vm['cmag_fit'].values\n",
    "    cm_lm = sub_vm['cmag_fit'].values\n",
    "    pos_um = np.ones(len(cd_um),dtype=int)*2\n",
    "    pos_lm = np.ones(len(cd_lm),dtype=int)*3\n",
    "    \n",
    "    return(np.concatenate([cd_hm,cd_um,cd_lm]), np.concatenate([cm_hm,cm_um,cm_lm]), np.concatenate([pos_hm,pos_um,pos_lm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowding_fit_data(df_HM, df_VM, 'sub-wlsubj158', 1) # 12 vals, 6+3+3, hm: lh*3, rh*3, vm: mean for 3 ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_fit_notused(df_HM, df_VM, sid, label, method = 'BFGS'):\n",
    "    from scipy.optimize import minimize\n",
    "    cd_cm = crowding_fit_data(df_HM, df_VM, sid, label)\n",
    "    params0 = [1.5, np.sqrt(1), np.sqrt(1), np.sqrt(1)] \n",
    "    r = minimize(crowding_loss, params0, args=cd_cm, method=method)\n",
    "    r.x[1] = r.x[1]**2\n",
    "    r.x[2] = r.x[2]**2\n",
    "    r.x[3] = r.x[3]**2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577be830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowding_fit(df_HM, df_VM, sid, label, method = 'BFGS'):\n",
    "    from scipy.optimize import minimize\n",
    "    cd_cm = crowding_fit_data(df_HM, df_VM, sid, label)\n",
    "    params0 = [1.5, np.log(1), np.log(1), np.log(1)] \n",
    "    r = minimize(crowding_loss, params0, args=cd_cm, method=method)\n",
    "    r.x[1] = np.exp(r.x[1])\n",
    "    r.x[2] = np.exp(r.x[2])\n",
    "    r.x[3] = np.exp(r.x[3])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit = dict(sid=[], label=[], d=[], g_h=[], g_u=[], g_l=[], loss=[])\n",
    "sids = np.unique(df_HM['sid'])\n",
    "for sid in sids:\n",
    "    for lbl in [1,2,3,4]:\n",
    "        r = crowding_fit(df_HM, df_VM, sid, lbl)\n",
    "        df_fit['sid'].append(sid)\n",
    "        df_fit['label'].append(lbl)\n",
    "        df_fit['d'].append(r.x[0])\n",
    "        df_fit['g_h'].append(r.x[1])\n",
    "        df_fit['g_u'].append(r.x[2])\n",
    "        df_fit['g_l'].append(r.x[3])\n",
    "        df_fit['loss'].append(r.fun)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b73441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fit_square = pd.DataFrame(df_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp = pd.DataFrame(df_fit)\n",
    "df_fit_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9caf298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp.groupby('label')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit_exp.groupby('label')['d'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH91_params.groupby('label')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b556d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b351530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp\n",
    "sns.violinplot(data=df_fit_exp,x='label',y='d',bw=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_fit_exp,x='label',y='loss',bw=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b936de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots based on HM measurements\n",
    "fig, axs = plt.subplots(4,2, figsize=(4,4), dpi=288, sharex=True, sharey=True)\n",
    "fig.subplots_adjust(0,0,1,1,0.1,0.3)\n",
    "\n",
    "for (lbl,axrow) in zip([1,2,3,4], axs):\n",
    "    df1 = df_HM[df_HM['label'] == lbl]\n",
    "    for (h,ax) in zip(['lh','rh'], axrow):\n",
    "        df2 = df1[df1['h'] == h]\n",
    "        loss = df2['loss']\n",
    "        vmag = df2['1d_vmag_fit']\n",
    "        cd = df2['CrowdingDistance_HM']\n",
    "        eccen = df2['eccen']\n",
    "        for (e,color) in zip([2.5,5,10],'rgb'):\n",
    "            ii = (eccen==e)\n",
    "            ax.scatter(vmag[ii], cd[ii], s=4*(2-loss[ii]/2000), c=color)\n",
    "            ax.plot([0,2],[0,2],'k-')\n",
    "            ax.set_title(f'{h} Label={lbl}', fontsize=6)\n",
    "            \n",
    "fig.text(0.5, -0.1, '1D Vmag Fit', ha='center', fontsize=8)\n",
    "fig.text(-0.1, 0.5, 'Crowding Distance', va='center', rotation='vertical', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45984446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85d10d",
   "metadata": {},
   "source": [
    "## linear regression: predict crowding distance based on vmag_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {1:[],2:[],3:[],4:[]}\n",
    "for subject, subject_data in df_HM.groupby(['sid']):\n",
    "    if (subject_data['b']>5).any():\n",
    "        continue\n",
    "    for h in ['lh','rh']:\n",
    "        ssdf = subject_data[subject_data['h'] == h]\n",
    "        for lbl in [1,2,3,4]:\n",
    "            sssdf = ssdf[ssdf['label'] == lbl]\n",
    "            x = sssdf['1d_vmag_fit'].values\n",
    "            y = sssdf['CrowdingDistance_HM'].values\n",
    "            (rss,coef) = cc.regression.fit_and_evaluate(x, y)\n",
    "            res[lbl].append(rss)\n",
    "            \n",
    "print({k:np.mean(v) for (k,v) in res.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a43647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = df_HM.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b']>5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "    for h in ['lh', 'rh']:\n",
    "        hemisphere_data = subject_data[subject_data['h'] == h]\n",
    "        \n",
    "        y1 = hemisphere_data[hemisphere_data['label'] == 1]['1d_vmag_fit'].values\n",
    "        y2 = hemisphere_data[hemisphere_data['label'] == 2]['1d_vmag_fit'].values\n",
    "        y3 = hemisphere_data[hemisphere_data['label'] == 3]['1d_vmag_fit'].values\n",
    "        y4 = hemisphere_data[hemisphere_data['label'] == 4]['1d_vmag_fit'].values\n",
    "        # since cd are the same for label 1,2,3,4\n",
    "        y5 = hemisphere_data[hemisphere_data['label'] == 1]['CrowdingDistance_HM'].values\n",
    "        \n",
    "        rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "        rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "        rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "        rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "        \n",
    "        rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "        best_fit_index = np.argmin(rss_values)\n",
    "        best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "        \n",
    "        subject_results[subject][h] = {\n",
    "            'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "            'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "            'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "            'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "            'best_fit': best_fit\n",
    "        }\n",
    "\n",
    "for subject, hemispheres in subject_results.items():\n",
    "    print(f\"Results for {subject}:\")\n",
    "    for h, results in hemispheres.items():\n",
    "        print(f\"  Hemisphere: {h}\")\n",
    "        print(f\"    RSS for label 1: {results['rss_label_1']}, Coefficient: {results['coef_label_1']}\")\n",
    "        print(f\"    RSS for label 2: {results['rss_label_2']}, Coefficient: {results['coef_label_2']}\")\n",
    "        print(f\"    RSS for label 3: {results['rss_label_3']}, Coefficient: {results['coef_label_3']}\")\n",
    "        print(f\"    RSS for label 4: {results['rss_label_4']}, Coefficient: {results['coef_label_4']}\")\n",
    "        print(f\"    The best fit is: {results['best_fit']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50834602",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_keys = ['rss_label_1', 'rss_label_2', 'rss_label_3', 'rss_label_4']\n",
    "\n",
    "data = [[\n",
    "    hemispheres[h][key]  \n",
    "    for subject, hemispheres in subject_results.items()\n",
    "    for h in ['lh', 'rh'] \n",
    "] for key in rss_keys]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, label=['Label 1', 'Label 2', 'Label 3', 'Label 4'])\n",
    "plt.xlabel('RSS Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of RSS Values for Each Label')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d15397",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, hemispheres in subject_results.items():\n",
    "    for h in ['lh', 'rh']:\n",
    "        rss_label_1_values.append(hemispheres[h]['rss_label_1'])\n",
    "        rss_label_2_values.append(hemispheres[h]['rss_label_2'])\n",
    "        rss_label_3_values.append(hemispheres[h]['rss_label_3'])\n",
    "        rss_label_4_values.append(hemispheres[h]['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subject_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c26b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_label_1, mean_rss_label_2, mean_rss_label_3, mean_rss_label_4]\n",
    "std_rss = [std_rss_label_1, std_rss_label_2, std_rss_label_3, std_rss_label_4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# standard error of the mean (SEM)\n",
    "n_subjects = len(subject_results) * 2  # for both hemispheres\n",
    "sem_rss = np.array(std_rss) / np.sqrt(n_subjects)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=sem_rss)\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_VM.groupby('sid') \n",
    "subject_results = {}\n",
    "\n",
    "for subject, subject_data in grouped:\n",
    "    if (subject_data['b'] > 5).any():\n",
    "        continue\n",
    "    subject_results[subject] = {}\n",
    "\n",
    "    y1 = subject_data[subject_data['label'] == 1]['1d_vmag_fit'].values\n",
    "    y2 = subject_data[subject_data['label'] == 2]['1d_vmag_fit'].values\n",
    "    y3 = subject_data[subject_data['label'] == 3]['1d_vmag_fit'].values\n",
    "    y4 = subject_data[subject_data['label'] == 4]['1d_vmag_fit'].values\n",
    "    y5 = subject_data[subject_data['label'] == 1]['CrowdingDistance_LVM'].values  # same for all labels\n",
    "\n",
    "    rss_y1, coef_y1 = cc.regression.fit_and_evaluate(y1, y5)\n",
    "    rss_y2, coef_y2 = cc.regression.fit_and_evaluate(y2, y5)\n",
    "    rss_y3, coef_y3 = cc.regression.fit_and_evaluate(y3, y5)\n",
    "    rss_y4, coef_y4 = cc.regression.fit_and_evaluate(y4, y5)\n",
    "\n",
    "    rss_values = [rss_y1, rss_y2, rss_y3, rss_y4]\n",
    "    best_fit_index = np.argmin(rss_values)\n",
    "    best_fit = ['label_1', 'label_2', 'label_3', 'label_4'][best_fit_index]\n",
    "\n",
    "    subject_results[subject] = {\n",
    "        'rss_label_1': rss_y1, 'coef_label_1': coef_y1,\n",
    "        'rss_label_2': rss_y2, 'coef_label_2': coef_y2,\n",
    "        'rss_label_3': rss_y3, 'coef_label_3': coef_y3,\n",
    "        'rss_label_4': rss_y4, 'coef_label_4': coef_y4,\n",
    "        'best_fit': best_fit\n",
    "    }\n",
    "\n",
    "\n",
    "for subject, results in subject_results.items():\n",
    "    print(f\"Results for {subject}:\")\n",
    "    print(f\"  RSS for label 1: {results['rss_label_1']}, Coefficient: {results['coef_label_1']}\")\n",
    "    print(f\"  RSS for label 2: {results['rss_label_2']}, Coefficient: {results['coef_label_2']}\")\n",
    "    print(f\"  RSS for label 3: {results['rss_label_3']}, Coefficient: {results['coef_label_3']}\")\n",
    "    print(f\"  RSS for label 4: {results['rss_label_4']}, Coefficient: {results['coef_label_4']}\")\n",
    "    print(f\"  The best fit is: {results['best_fit']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427245ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_label_1_values = []\n",
    "rss_label_2_values = []\n",
    "rss_label_3_values = []\n",
    "rss_label_4_values = []\n",
    "\n",
    "for subject, results in subject_results.items():\n",
    "    rss_label_1_values.append(results['rss_label_1'])\n",
    "    rss_label_2_values.append(results['rss_label_2'])\n",
    "    rss_label_3_values.append(results['rss_label_3'])\n",
    "    rss_label_4_values.append(results['rss_label_4'])\n",
    "\n",
    "mean_rss_label_1 = np.mean(rss_label_1_values)\n",
    "mean_rss_label_2 = np.mean(rss_label_2_values)\n",
    "mean_rss_label_3 = np.mean(rss_label_3_values)\n",
    "mean_rss_label_4 = np.mean(rss_label_4_values)\n",
    "\n",
    "std_rss_label_1 = np.std(rss_label_1_values)\n",
    "std_rss_label_2 = np.std(rss_label_2_values)\n",
    "std_rss_label_3 = np.std(rss_label_3_values)\n",
    "std_rss_label_4 = np.std(rss_label_4_values)\n",
    "\n",
    "print(f\"Mean RSS for Label 1: {mean_rss_label_1:.4f}, Standard Deviation: {std_rss_label_1:.4f}\")\n",
    "print(f\"Mean RSS for Label 2: {mean_rss_label_2:.4f}, Standard Deviation: {std_rss_label_2:.4f}\")\n",
    "print(f\"Mean RSS for Label 3: {mean_rss_label_3:.4f}, Standard Deviation: {std_rss_label_3:.4f}\")\n",
    "print(f\"Mean RSS for Label 4: {mean_rss_label_4:.4f}, Standard Deviation: {std_rss_label_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rss = [mean_rss_label_1, mean_rss_label_2, mean_rss_label_3, mean_rss_label_4]\n",
    "std_rss = [std_rss_label_1, std_rss_label_2, std_rss_label_3, std_rss_label_4]\n",
    "\n",
    "labels = ['V1', 'V2', 'V3', 'V4']\n",
    "\n",
    "# standard error of the mean (SEM)\n",
    "n_subjects = len(subject_results)\n",
    "sem_rss = np.array(std_rss) / np.sqrt(n_subjects)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, mean_rss, yerr=sem_rss)\n",
    "\n",
    "plt.ylabel('RSS (Residual Sum of Squares)')\n",
    "plt.title('Mean RSS with Standard Error Across Visual Areas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dce51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
